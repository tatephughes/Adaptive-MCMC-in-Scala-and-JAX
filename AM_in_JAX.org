#+TITLE: AM in Python-JAX
#+PROPERTY: header-args :tangle AM_in_JAX.py
#+auto_tangle: t

:BOILERPLATE:
#+BIBLIOGRAPHY: Bibliography.bib
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [letterpaper]
#+OPTIONS: toc:nil
#+LATEX_HEADER: \usepackage{amsmath,amsfonts,amsthm,amssymb,bm,bbm,tikz,tkz-graph}
#+LATEX_HEADER: \usetikzlibrary{arrows}
#+LATEX_HEADER: \usetikzlibrary{bayesnet}
#+LATEX_HEADER: \usetikzlibrary{matrix}
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \newtheorem{theorem}{Theorem}[section]
#+LATEX_HEADER: \newtheorem{corollary}[theorem]{Corollary}
#+LATEX_HEADER: \newtheorem{lemma}[theorem]{Lemma}
#+LATEX_HEADER: \newtheorem{definition}[theorem]{Definition}
#+LATEX_HEADER: \newtheorem*{remark}{Remark}
#+LATEX_HEADER: \DeclareMathOperator{\E}{\mathbb E}}
#+LATEX_HEADER: \DeclareMathOperator{\prob}{\mathbb P}
#+LATEX_HEADER: \DeclareMathOperator{\var}{\mathbb V\mathrm{ar}}
#+LATEX_HEADER: \DeclareMathOperator{\cov}{\mathbb C\mathrm{ov}}
#+LATEX_HEADER: \DeclareMathOperator{\cor}{\mathbb C\mathrm{or}}
#+LATEX_HEADER: \DeclareMathOperator{\normal}{\mathcal N}
#+LATEX_HEADER: \DeclareMathOperator{\invgam}{\mathcal{IG}}
#+LATEX_HEADER: \newcommand*{\mat}[1]{\bm{#1}}
#+LATEX_HEADER: \newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
#+LATEX_HEADER: \renewcommand*{\vec}[1]{\boldsymbol{\mathbf{#1}}}
#+EXPORT_EXCLUDE_TAGS: noexport
:END:

This file _is_ the source code; everything below gets 'tangled' into ~AM_in_JAX.py~.

* Boilerplate

** Shabang

#+begin_src python :session example :results output
#!/usr/bin/env python3
#+end_src

#+RESULTS:

** Imports

#+begin_src python :session example :results none
import matplotlib.pyplot as plt
import jax
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand
from jax.numpy.linalg import solve, qr, norm, eig, eigh, inv, cholesky
import time
from AM_in_JAX_tests import *
import csv
#+end_src

** Test Imports

#+begin_src python :session example :results none :tangle AM_in_JAX_tests.py
import matplotlib.pyplot as plt
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand
import jax.scipy.stats as stat
from jax import vmap
from jax.numpy.linalg import solve, qr, norm, eig, inv, cholesky
import jax
import time
from AM_in_JAX import *
#+end_src

** Enable 64-bit precision

This significantly slows the code on most GPUs.

Without this however, the precision is not enough at higher dimensions.

#+begin_src python :session example :results none
jax.config.update('jax_enable_x64', True)
#+end_src

* Core Functions

Let's assume that the ~state~ is a tuple with four elements, instead of it's own class. JAX reads this as a PyTree, which it is happy to preform operations on (which wouldn't be the case if I made ~state~ a class like in the Scala version)

More specificaly, ~state = (j, x, x_mean, prop_cov)~.

** ~try_accept~

This function takes a state, a proposed move, and a log probabilty, and returns the next state, using the probability as expected.

#+begin_src python :session example :results none
def try_accept(state, prop, alpha, key):

  j       = state[0]
  x       = state[1]
  x_mean  = state[2]
  prop_cov   = state[3]
  d       = x.shape[0]
  
  log_prob = jnp.minimum(0.0, alpha)

  u = rand.uniform(key)

  x_new, is_accepted = jl.cond((jnp.log(u) < log_prob),
                               0, lambda _: (prop, True),
                               0, lambda _: (x, False))

  x_mean_new = x_mean*(j-1)/j  + x_new/j

  prop_cov_new = jl.cond(j <= 2*d,
                         j,
                         lambda t: prop_cov,
                         j,
                         lambda t: prop_cov*(t-1)/t + (t*jnp.outer(x_mean,x_mean) - (t+1)*jnp.outer(x_mean_new,x_mean_new) + jnp.outer(x_new,x_new) + 0.01*jnp.identity(d))*5.6644/(t*d))
  
  # NOTE: seems inefficient to construct a diagonal identity matrix like this, I would imagine there is a better way to do this
  return((j + 1,
          x_new,
          x_mean_new,
          prop_cov_new,
          is_accepted))
#+end_src

*** ~test_try_accept~

The below code block does a few tests on the ~try_accept~ function. If the tests pass, it will return ~True~, otherwise it will throw an error.

#+begin_src python :session example :results none :tangle AM_in_JAX_tests.py
def test_try_accept():
    
    d = 10
    key = jax.random.PRNGKey(seed=2)
    keys = rand.split(key,10000)
    state0 = (0, jnp.zeros(10), jnp.zeros(10), jnp.identity(10), False)
    prop = jnp.ones(10)
    
    '''
    Test 1:
    if alpha=log(0.5), then the function should accept approx. 50% of the proposals
    '''
    assert jnp.abs(jnp.mean(jl.map(lambda x: try_accept(state0, prop, jnp.log(0.5), x), keys)[4]) - 0.5 < 0.1), "Accepting at unexpected rate"

    '''
    Test 1.5:
    if alpha=-0.33333333, then the function should accept approx. 0.7165 of the proposals
    '''
    assert jnp.abs(jnp.mean(jl.map(lambda x: try_accept(state0, prop, -0.3333333, x), keys)[4]) - 0.7165 < 0.1), "Accepting at unexpected rate"

    '''
    Test 2:
    if alpha=log(0)=-inf, then the function should never accept, and should return the
    proposed value
    '''
    assert jnp.all(try_accept(state0, prop, jnp.log(0), key)[1]==jnp.zeros(10)), "Not rejecting proposal"

    '''
    Test 3:
    if alpha=log(1)=0 then the function should always accept, and should return the
    proposed value
    '''
    assert jnp.all(try_accept(state0, prop, jnp.log(1), key)[1]==prop), "Not accepting proposal"

    '''
    Test 4:
    No matter what, j should increment by exactly 1
    '''
    assert jnp.all(jl.map(lambda x: try_accept(state0, prop, jnp.log(0.5), x), keys)[0]==1), "Index not correctly implemented"

    '''
    Test 5:
    When it accepts, the x_sum should increase accordingly
    '''
    assert jnp.all(try_accept(state0, prop, jnp.log(1), key)[2]==prop), "Not increased x_sum"
    assert jnp.all(try_accept(state0, prop, jnp.log(0), key)[2]==jnp.zeros(10)), "Not increased x_sum"

    '''
    Test 6:
    When it accepts, the xxt_sum should increase accordingly
    '''
    assert jnp.all(try_accept(state0, prop, jnp.log(1), key)[3]==jnp.identity(10) + jnp.outer(prop, prop)), "Not increased xxt_sum"
    assert jnp.all(try_accept(state0, prop, jnp.log(0), key)[3]==jnp.identity(10)), "Not increased xxt_sum"

    return True
#+end_src

** ~adapt_step~

The actually adaptive part, implementing a step with proposal
\begin{align*}
q(\vec X_t^* \mid \vec X_0, \dots, X_{t-1}) \sim \mathcal N_d (\vec X_{t-1}, \mat C_t),
\end{align*}
where
\begin{align*}
C_j =
\begin{cases}
\mat s_d \Sigma_0 &\quad j \leq j_0,\\
s_d(\Sigma_{j-1} + \epsilon \mat I_d) &\quad j>j_0=2d.
\end{cases}
\end{align*}

#+begin_src python :session example :results none
def adapt_step(state, q, r, key):

    j        = state[0] # this is an int32, not big enough when i square it below!
    x        = state[1]
    prop_cov = state[3]
    d        = x.shape[0]

    keys = rand.split(key,2)
    
    prop = rand.multivariate_normal(keys[0], x, prop_cov)

    # Compute the log acceptance probability
    alpha = 0.5 * (x.T @ (solve(r, q.T @ x))
                   - (prop.T @ solve(r, q.T @ prop)))

    return(try_accept(state, prop, alpha, keys[1]))
#+end_src

*** ~test_adapt_step~

#+begin_src python :session example :results none :tangle AM_in_JAX_tests.py
def test_adapt_step():

    d = 2
    n = 100000
    key = jax.random.PRNGKey(seed=1)
    keys = rand.split(key,n)
    # this state was chosen being close to an actual state of the adaptive chain
    state = (100, jnp.zeros(2), jnp.array([-80.0,-5.0]), jnp.array([[260.0,100.0],[100.0,150.0]]), False)
    sigma = jnp.array([[2.0,1.0],[1.0,2.0]])
    Q, R = qr(sigma)
    
    '''
    Test 1:
    From a (hypothetical) progressed point, the result should be approximately distributed with a N(0,sigma) distribution.
    '''
    def step(carry, _):
        nextstate = adapt_step(carry, Q, R, keys[carry[0]])[0]
        return(nextstate, nextstate)
    
    assert norm(cov(jl.scan(step, state, jnp.zeros(n))[1][1]) - sigma) < 0.2, "adap_stepr not producing sample sufficiently close to the target distribution"

    
    return True
#+end_src


*** Covariance function

Since there isn't one built-in anywhere as far as I can tell, this is a simple function to compute the covariance matrix of a sample.

#+begin_src python :session example :results none
def cov(sample):
    
    means = jnp.mean(sample, axis=0)

    deviations = sample - means
    
    N = sample.shape[0]
    
    covariance = jnp.dot(deviations.T, deviations) / (N - 1)
    
    return covariance
#+end_src


* ~effectiveness~

** TODO Fix the eigenvalue computation with a change of coordinate

#+begin_src python :session example :results none
def effectiveness(sigma, sigma_j):

    d = sigma.shape[0]
    
    sigma_j_decomp = eigh(sigma_j)
    sigma_decomp = eigh(sigma)
    
    rootsigmaj = sigma_j_decomp[1] @ jnp.diag(jnp.sqrt(sigma_j_decomp[0])) @ inv(sigma_j_decomp[1])
    rootsigmainv = inv(sigma_decomp[1] @ jnp.diag(jnp.sqrt(sigma_decomp[0])) @ inv(sigma_decomp[1]))

    # the below line relies on the ~eig~ function which doesn't work on GPUs
    lam = eig(rootsigmaj @ rootsigmainv)[0]
    lambdaminus2sum = sum(1/(lam*lam))
    lambdainvsum = sum(1/lam)

    b = (d * (lambdaminus2sum / (lambdainvsum*lambdainvsum))).real

    return b
#+end_src


* plotting

Exactly as in the Scala version, simply plots the trace of the first coordinate of the given sample, and saves it to a file.

#+begin_src python :session example :results none
def plotter(sample, file_path, d):
    
    first = sample[:,0]
    plt.figure(figsize=(590/96,370/96))
    plt.plot(first)
    plt.title(f'Trace plot of the first coordinate, d={d}')
    plt.xlabel('Step')
    plt.ylabel('First coordinate value')
    plt.grid(True)
    plt.savefig(file_path, dpi=96)

#+end_src


* Compute time vs. dimension

** ~run_with_complexity~

#+begin_src python :session example :results none
def run_with_complexity(sigma_d, key):

    Q, R = qr(sigma_d) # take the QR decomposition of sigma

    # since I'm timing, this is not a pure function, so
    # it won't work completely through JAX.

    d = sigma_d.shape[0]
    
    # these numbers get good results up to d=100
    n = 10000
    thinrate = 10
    burnin = 1000000

    keys = rand.split(key, n + burnin + 1)
    state0 = (1, jnp.zeros(d), jnp.zeros(d), jnp.identity(d)/d, False)
    
    def step(carry, key):
        nextstate = thinned_step(thinrate, carry, Q, R, key)
        return(nextstate, nextstate)

    start_time = time.time()
    
    # inital state, after burnin
    start_state = jl.fori_loop(1, burnin+1, lambda i,x: adapt_step(x, Q, R, keys[i]), state0)
    # the sample
    am_sample = jl.scan(step, start_state, keys[burnin+1:])[1]

    end_time = time.time()
    duration = time.time()-start_time
    
    sigma_j = cov(am_sample[1])
    
    b = effectiveness(sigma_d,sigma_j)

    return n, thinrate, burnin, duration, float(b) # making it into a normal float for readability
#+end_src

** ~compute_time_graph~

#+begin_src python :session example :results none
def compute_time_graph(sigma, csv_file):
    
    d = 5 #sigma.shape[0]

    key = rand.PRNGKey(seed=1)
    keys = rand.split(key, d)
    
    x = range(1, d+1)
    y = jnp.array([run_with_complexity(sigma[:i,:i], keys[i]) for i in x if print(i) or True])

    with open(csv_file, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerows(y)
#+end_src


* ~thinned_step~

Thinning as I've done it above is not memory efficient; it stores all ~n~ states and only thins right at the end. Instead, the function ~thinned_step~ uses a fori_loop to 'jump' steps, which JAX knows how to garbage collect. This is especially important for high dimensional samples, as below.

#+begin_src python :session example :results none
def thinned_step(thinrate, state, q, r, key):

    keys = rand.split(key,thinrate)

    # I think this should scan over the keys!
    return jl.fori_loop(0, thinrate, (lambda i, x: adapt_step(x, q, r, keys[i])), state)
#+end_src

** ~test_thinned_step~

#+begin_src python :session example :results none :tangle AM_in_JAX_tests.py
def test_thinned_step():

    d = 2
    n = 1000
    thinrate = 10
    key = jax.random.PRNGKey(seed=1)
    keys = rand.split(key,n)
    # this state was chosen being close to an actual state of the adaptive chain
    state = (100, jnp.zeros(2), jnp.array([-80.0,-5.0]), jnp.array([[260.0,100.0],[100.0,150.0]]), False)
    sigma = jnp.array([[2.0,1.0],[1.0,2.0]])
    Q, R = qr(sigma)
    
    '''
    Test 1:
    the index of a state should increase by thinrate
    '''
    assert (thinned_step(thinrate, state, Q, R, keys[0])[0] == 100+thinrate), "thinned_step not correctly incrementing step count"

    return True
  
#+end_src


* ~main~

Due to memory constraints and garbage collection not being quite as magical, we do burn-in seperately to the main sampling.

#+begin_src python :session example :results none
def main(d=10, n=100000, thinrate=10, burnin=10000, file="Figures/adaptive_trace_JAX.png"):

    # the actual number of iterations is n*thin + burnin
    # computed_size = n*thinrate + burnin

    # keys for PRNG
    key = jax.random.PRNGKey(seed=1)
    keys = rand.split(key, n + burnin + 1)
    
    # create a chaotic variance matrix to target
    M = rand.normal(keys[0], shape = (d,d))
    sigma = M.T @ M
    Q, R = qr(sigma) # take the QR decomposition of sigma

    # initial state before burn-in
    state0 = (1, jnp.zeros(d), jnp.zeros(d), ((0.1)**2) * jnp.identity(d)/d, False)

    # JAX's ~scan~ isn't quite ~iterate~, so this is a 'dummy'
    # function with an unused argument to call thinned_step for the
    # actually used samples
    # NOTE: this comment may be out of date now that I am scanning over the keys
    def step(carry, key):
        nextstate = thinned_step(thinrate, carry, Q, R, key)
        return(nextstate, nextstate)

    start_time = time.time()
    
    # inital state, after burnin
    start_state = jl.fori_loop(1, burnin+1, lambda i,x: adapt_step(x, Q, R, keys[i]), state0)

    # the sample
    am_sample = jl.scan(step, start_state, keys[burnin+1:])[1]

    end_time = time.time()
    duration = time.time()-start_time
    
    # the empirical covariance of the sample
    sigma_j = cov(am_sample[1])
    b = effectiveness(sigma,sigma_j)

    # the tiume of the computation in seconds
    
    print(f"The true variance of x_1 is {sigma[0,0]}")
    print(f"The empirical sigma value is {sigma_j[0,0]}")
    print(f"The b value is {b}")
    print(f"The computation took {duration} seconds")

    plotter(am_sample[1], file, d)
    
    return am_sample

#+end_src

The entry point for if the code is run in a console.

#+begin_src python :session example :results graphics file output :file Figures/adaptive_trace_jax.png :height 200
if __name__ == "__main__":
    #test_try_accept()
    #test_init_step()
    #test_adapt_step()
    #test_AM_hstep()
    #test_thinned_step()
    main()
    #or high dimensions
    #main(d=100, n=10000, thinrate=100, burnin=1000000, file ="Figures/adaptive_trace_JAX_high_d.png")
    #matrix = []
    #with open('./data/chaotic_variance.csv', 'r', newline='') as file:
    #    reader = csv.reader(file)
    #    for row in reader:
    #        matrix.append([float(item) for item in row])
    #sigma = jnp.array(matrix)
    #compute_time_graph(sigma, "data/JAX_compute_times.csv")
#+end_src


* Test Area

#+begin_src python :session example :result output :tangle no
d=2
n=10000
thinrate=10
burnin=10000

start_time = time.time()
# the actual number of iterations is n*thin + burnin
# computed_size = n*thinrate + burnin

# keys for PRNG
key = jax.random.PRNGKey(seed=1)
keys = rand.split(key, n+burnin+1)

# create a chaotic variance matrix to target
M = rand.normal(keys[0], shape = (d,d))
sigma = M.T @ M
Q, R = qr(sigma) # take the QR decomposition of sigma

# initial state before burn-in
state0 = (1, jnp.zeros(d), jnp.zeros(d), ((0.1)**2)*jnp.identity(d)/d, False)

# JAX's ~scan~ isn't quite ~iterate~, so this is a 'dummy'
# function with an unused argument to call thinned_step for the
# actually used samples
# NOTE: this comment may be out of date now that I am scanning over the keys
def step(carry, key):
        nextstate = thinned_step(thinrate,carry, Q, R, key)
        return(nextstate, nextstate)

# inital state, after burnin
# again, I should probably scan here
start_state = jl.fori_loop(1, burnin+1, lambda i,x: adapt_step(x, Q, R, keys[i]), state0)

# the sample
am_sample = jl.scan(step, start_state, keys[burnin+1:])[1]

# the tiume of the computation in seconds
end_time = time.time()
duration = time.time() - start_time
#+end_src

#+RESULTS:
: None
