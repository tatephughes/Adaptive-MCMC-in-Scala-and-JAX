#+TITLE: AM in Python-JAX
#+PROPERTY: header-args :tangle AM_in_JAX.py
#+auto_tangle: t
#+BIBLIOGRAPHY: Bibliography.bib
#+LATEX_HEADER: \usepackage{amsmath,amsfonts,amssymb}

This file _is_ the source code; everything below gets 'tangled' into ~AM_in_JAX.py~.

* Boilerplate

** Shabang

#+begin_src python :session example :results output
#!/usr/bin/env python3
#+end_src

#+RESULTS:

** Imports

#+begin_src python :session example :results none
import matplotlib.pyplot as plt
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand
import jax.scipy.stats as stat
from jax import vmap
from jax.numpy.linalg import solve, qr, norm, eig, inv, cholesky
import jax
import time
from AM_in_JAX_tests import *
#+end_src

** Test Imports

#+begin_src python :session example :results none :tangle AM_in_JAX_tests.py
import matplotlib.pyplot as plt
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand
import jax.scipy.stats as stat
from jax import vmap
from jax.numpy.linalg import solve, qr, norm, eig, inv, cholesky
import jax
import time
from AM_in_JAX import *
#+end_src

** Enable 64-bit precision

#+begin_src python :session example :results none
jax.config.update('jax_enable_x64', True)
#+end_src


* ~AM_step~

AM_step is fragmented into four functions, in contrast to the Scala version.

Let's assume that the ~state~ is a tuple with four elements, instead of it's own class. JAX reads this as a PyTree, which it is happy to preform operations on (which wouldn't be the case if I made ~state~ a class like in the Scala version)

More specificaly, ~state = (j, x, x_sum, xxt_sum)~.

** ~try_accept~

This function takes a state, a proposed move, and a log probabilty, and returns the next state, using the probability as expected.

#+begin_src python :session example :results none
def try_accept(state, prop, alpha, key):

  j       = state[0]
  x       = state[1]
  x_sum   = state[2]
  xxt_sum = state[3]
  d       = x.shape[0]
  
  log_prob = jnp.minimum(0.0, alpha)

  u = rand.uniform(key)

  #new_x = prop if (jnp.log(u) < log_prob) else x

  new_x, is_accepted = jl.cond((jnp.log(u) < log_prob),
                  0, lambda _: (prop, True),
                  0, lambda _: (x, False))
  
  return((j + 1,
          new_x,
          x_sum + new_x,
          xxt_sum + jnp.outer(new_x, new_x),
          is_accepted))
#+end_src

*** ~test_try_accept~

The below code block does a few tests on the ~try_accept~ function. If the tests pass, it will return ~True~, otherwise it will throw an error.

#+begin_src python :session example :results none :tangle AM_in_JAX_tests.py
def test_try_accept():
    
    d = 10
    key = jax.random.PRNGKey(seed=2)
    keys = rand.split(key,10000)
    state0 = (0, jnp.zeros(10), jnp.zeros(10), jnp.identity(10), False)
    prop = jnp.ones(10)
    
    '''
    Test 1:
    if alpha=log(0.5), then the function should accept approx. 50% of the proposals
    '''
    assert jnp.abs(jnp.mean(jl.map(lambda x: try_accept(state0, prop, jnp.log(0.5), x), keys)[4]) - 0.5 < 0.1), "Accepting at unexpected rate"

    '''
    Test 1.5:
    if alpha=-0.33333333, then the function should accept approx. 0.7165 of the proposals
    '''
    assert jnp.abs(jnp.mean(jl.map(lambda x: try_accept(state0, prop, -0.3333333, x), keys)[4]) - 0.7165 < 0.1), "Accepting at unexpected rate"

    '''
    Test 2:
    if alpha=log(0)=-inf, then the function should never accept, and should return the
    proposed value
    '''
    assert jnp.all(try_accept(state0, prop, jnp.log(0), key)[1]==jnp.zeros(10)), "Not rejecting proposal"

    '''
    Test 3:
    if alpha=log(1)=0 then the function should always accept, and should return the
    proposed value
    '''
    assert jnp.all(try_accept(state0, prop, jnp.log(1), key)[1]==prop), "Not accepting proposal"

    '''
    Test 4:
    No matter what, j should increment by exactly 1
    '''
    assert jnp.all(jl.map(lambda x: try_accept(state0, prop, jnp.log(0.5), x), keys)[0]==1), "Index not correctly implemented"

    '''
    Test 5:
    When it accepts, the x_sum should increase accordingly
    '''
    assert jnp.all(try_accept(state0, prop, jnp.log(1), key)[2]==prop), "Not increased x_sum"
    assert jnp.all(try_accept(state0, prop, jnp.log(0), key)[2]==jnp.zeros(10)), "Not increased x_sum"

    '''
    Test 6:
    When it accepts, the xxt_sum should increase accordingly
    '''
    assert jnp.all(try_accept(state0, prop, jnp.log(1), key)[3]==jnp.identity(10) + jnp.outer(prop, prop)), "Not increased xxt_sum"
    assert jnp.all(try_accept(state0, prop, jnp.log(0), key)[3]==jnp.identity(10)), "Not increased xxt_sum"

    return True
#+end_src

** ~init_step~

The procedure for taking a step forward when $j\leq2d$. This is equivalent to a random walk metropolis step with proposal $\mathcal N(x,d^{-1}I)$.

#+begin_src python :session example :results none
def init_step(state,q,r,key):

    j       = state[0]
    x       = state[1]
    x_sum   = state[2]
    xxt_sum = state[3]
    d       = x.shape[0]

    keys = rand.split(key,3)
    z = rand.normal(keys[0], shape=(d,))
    
    # The propasal distribution is N(x,1/d) for this first stage
    prop = z/d + x
    
    # Compute the log acceptance probability
    alpha = 0.5 * (x @ (solve(r, q.T @ x)) - (prop @ solve(r, q.T @ prop)))
    
    return(try_accept(state, prop, alpha, keys[1]))    
#+end_src

*** ~test_init_step~

#+begin_src python :session example :results none :tangle AM_in_JAX_tests.py
def test_init_step():

    # this doesn't take long, but I feel it still takes too long.
    # I don't want to get into the habit of writing tests with
    # this amount of computation.
    
    d = 2
    n = 100000
    key = jax.random.PRNGKey(seed=1)
    keys = rand.split(key,n)
    state0 = (0, jnp.zeros(2), jnp.zeros(2), jnp.identity(2), False)
    sigma = jnp.array([[2.0,1.0],[1.0,2.0]])
    Q, R = qr(sigma)
        
    '''
    Test 1:
    From state0, the result should be approximately distributed with a N(0,sigma) distribution;
    it should be a standard Random Walk metropolis
    '''
    def step(carry, _):
        nextstate = init_step(carry, Q, R, keys[carry[0]])
        return(nextstate, nextstate)
    
    assert norm(cov(jl.scan(step, state0, jnp.zeros(n))[1][1]) - sigma) < 0.2, "init_step not producing sample sufficiently close to the target distribution"

    return True
#+end_src

** ~adap_step~

The actually adaptive part, implementing a step with proposal

$$\begin{aligned}
q(x,\cdot)\sim(1-\beta)\mathcal N(x,(2.38)^2\Sigma_j/d)+\beta\mathcal N(x,(0.1)^2I_d/d)
\end{aligned}$$

where $\Sigma_j$ is the current empirical covariance matrix.

#+begin_src python :session example :results none
def adapt_step(state, q, r, key):

    j       = state[0] # this is an int32, not big enough when i square it below!
    x       = state[1]
    x_sum   = state[2]
    xxt_sum = state[3]
    d       = x.shape[0]

    keys = rand.split(key,3)

    z = rand.normal(keys[0], shape=(d,))
    
    emp_var = (xxt_sum/j - jnp.outer(x_sum, x_sum)/(j**2))

    u = rand.uniform(keys[1])
    
    prop = jl.cond(u < 0.95,
                   x,
                   lambda y: rand.multivariate_normal(keys[2], y,
                                                 emp_var * (2.38**2/d)),
                   x,
                   lambda y:((rand.normal(keys[2], shape=(d,))/(100*d) + y)))
    
    # Compute the log acceptance probability
    alpha = 0.5 * (x.T @ (solve(r, q.T @ x))
                   - (prop.T @ solve(r, q.T @ prop)))
    
    return(try_accept(state, prop, alpha, keys[2]), emp_var)
#+end_src

*** ~test_adapt_step~

#+begin_src python :session example :results none :tangle AM_in_JAX_tests.py
def test_adapt_step():

    d = 2
    n = 100000
    key = jax.random.PRNGKey(seed=1)
    keys = rand.split(key,n)
    # this state was chosen being close to an actual state of the adaptive chain
    state = (100, jnp.zeros(2), jnp.array([-80.0,-5.0]), jnp.array([[260.0,100.0],[100.0,150.0]]), False)
    sigma = jnp.array([[2.0,1.0],[1.0,2.0]])
    Q, R = qr(sigma)
    
    '''
    Test 1:
    From a (hypothetical) progressed point, the result should be approximately distributed with a N(0,sigma) distribution.
    '''
    def step(carry, _):
        nextstate = adapt_step(carry, Q, R, keys[carry[0]])[0]
        return(nextstate, nextstate)
    
    assert norm(cov(jl.scan(step, state, jnp.zeros(n))[1][1]) - sigma) < 0.2, "adap_stepr not producing sample sufficiently close to the target distribution"

    
    return True
#+end_src

** ~AM_step~

Does one of the above two methods, depending on how far along the chain is.

#+begin_src python :session example :results none
def AM_step(state, q, r, key):

    j       = state[0]
    x       = state[1]
    x_sum   = state[2]
    xxt_sum = state[3]
    d       = x.shape[0]

    return(jl.cond(j <= 2*d,
                   state,
                   lambda y: init_step(y, q, r, key),
                   state,
                   lambda y: adapt_step(y, q, r, key)[0]))
#+end_src

*** ~test_AM_step~

#+begin_src python :session example :results none :tangle AM_in_JAX_tests.py
def test_AM_step():

    d = 2
    n = 100000
    key = jax.random.PRNGKey(seed=1)
    keys = rand.split(key,n)
    state0 = (0, jnp.zeros(2), jnp.zeros(2), jnp.identity(2), False)
    sigma = jnp.array([[2.0,1.0],[1.0,2.0]])
    Q, R = qr(sigma)
        
    '''
    Test 1:
    Similarily to the init_step test, from state0, the result should be approximately distributed with a N(0,sigma) distribution.
    '''
    def step(carry, _):
        nextstate = AM_step(carry, Q, R, keys[carry[0]])
        return(nextstate, nextstate)
    
    assert norm(cov(jl.scan(step, state0, jnp.zeros(n))[1][1]) - sigma) < 0.2, "init_step not producing sample sufficiently close to the target distribution"
    
    return True
#+end_src


*** Covariance function

Since there isn't one built-in anywhere as far as I can tell, this is a simple function to compute the covariance matrix of a sample.

#+begin_src python :session example :results none
def cov(sample):
    
    means = jnp.mean(sample, axis=0)

    deviations = sample - means
    
    N = sample.shape[0]
    
    covariance = jnp.dot(deviations.T, deviations) / (N - 1)
    
    return covariance
#+end_src


* ~effectiveness~

#+begin_src python :session example :results none
def effectiveness(sigma, sigma_j):

    d = sigma.shape[0]
    
    sigma_j_decomp = eig(sigma_j)
    sigma_decomp = eig(sigma)
    
    rootsigmaj = sigma_j_decomp[1] @ jnp.diag(jnp.sqrt(sigma_j_decomp[0])) @ inv(sigma_j_decomp[1])
    rootsigmainv = inv(sigma_decomp[1] @ jnp.diag(jnp.sqrt(sigma_decomp[0])) @ inv(sigma_decomp[1]))
    
    lam = eig(rootsigmaj @ rootsigmainv)[0]
    lambdaminus2sum = sum(1/(lam*lam))
    lambdainvsum = sum(1/lam)

    b = (d * (lambdaminus2sum / (lambdainvsum*lambdainvsum))).real

    return b
#+end_src


* plotting

Exactly as in the Scala version, simply plots the trace of the first coordinate of the given sample, and saves it to a file.

#+begin_src python :session example :results none
def plotter(sample, file_path, d):
    
    first = sample[:,0]
    plt.figure(figsize=(590/96,370/96))
    plt.plot(first)
    plt.title(f'Trace plot of the first coordinate, d={d}')
    plt.xlabel('Step')
    plt.ylabel('First coordinate value')
    plt.grid(True)
    plt.savefig(file_path, dpi=96)

#+end_src



* ~thinned_step~

Thinning as I've done it above is not memory efficient; it stores all ~n~ states and only thins right at the end. Instead, the function ~thinned_step~ uses a fori_loop to 'jump' steps, which JAX knows how to garbage collect. This is especially important for high dimensional samples, as below.

#+begin_src python :session example :results none
def thinned_step(thinrate, state, q, r, key):

    keys = rand.split(key,thinrate)
    
    return jl.fori_loop(0, thinrate, (lambda i, x: AM_step(x, q, r, keys[i])), state)
#+end_src

** ~test_thinned_step~

#+begin_src python :session example :results none :tangle AM_in_JAX_tests.py
def test_thinned_step():

    d = 2
    n = 1000
    thinrate = 10
    key = jax.random.PRNGKey(seed=1)
    keys = rand.split(key,n)
    # this state was chosen being close to an actual state of the adaptive chain
    state = (100, jnp.zeros(2), jnp.array([-80.0,-5.0]), jnp.array([[260.0,100.0],[100.0,150.0]]), False)
    sigma = jnp.array([[2.0,1.0],[1.0,2.0]])
    Q, R = qr(sigma)
    
    '''
    Test 1:
    the index of a state should increase by thinrate
    '''
    assert (thinned_step(thinrate, state, Q, R, keys[0])[0] == 100+thinrate), "thinned_step not correctly incrementing step count"

    return True
  
#+end_src


* ~main~

Due to memory constraints and garbage collection not being wuite as magical, we do burn-in seperately to the main sampling.

#+begin_src python :session example :results none
def main(d=10, n=10000, thinrate=10, burnin=1000):

    start_time = time.time()

    # the actual number of iterations is n*thin + burnin
    computed_size = n*thinrate + burnin

    # keys for PRNG
    key = jax.random.PRNGKey(seed=2)
    keys = rand.split(key, computed_size+1)
    
    # create a chaotic variance matrix to target
    M = rand.normal(keys[0], shape = (d,d))
    sigma = M.T @ M
    Q, R = qr(sigma) # take the QR decomposition of sigma

    # initial state before burn-in
    state0 = (1, jnp.zeros(d), jnp.zeros(d), jnp.identity(d), False)

    # JAX's ~scan~ isn't quite ~iterate~, so this is a 'dummy'
    # function with an unused argument to call thinned_step for the
    # actually used samples
    def step(carry, _):
        nextstate = thinned_step(thinrate, carry, Q, R, keys[carry[0]])
        return(nextstate, nextstate)

    # inital state, after burnin
    start_state = jl.fori_loop(1, burnin, lambda i,x: AM_step(x, Q, R, keys[i]), state0)
    # this will take a while to run, but once it's done there is only 10000 more to compute;

    # the sample
    am_sample = jl.scan(step, start_state, jnp.zeros(n))[1]

    # the empirical covariance of the sample
    sigma_j = cov(am_sample[1])
    b = effectiveness(sigma,sigma_j)

    # the tiume of the computation in seconds
    end_time = time.time()
    duration = time.time()-start_time
    
    print(f"The true variance of x_1 is {sigma[0,0]}")
    print(f"The empirical sigma value is {sigma_j[0,0]}")
    print(f"The b value is {b}")
    print(f"The computation took {duration} seconds")

    plotter(am_sample[1], "Figures/adaptive_trace_jax_high_d.png", d)
    
    return am_sample

#+end_src

The entry point for if the code is run in a console.

#+begin_src python :session example :results graphics file output :file Figures/adaptive_trace_jax.png :height 200
if __name__ == "__main__":
    test_try_accept()
    test_init_step()
    test_adapt_step()
    test_AM_step()
    test_thinned_step()
    main(d=100, n=100000, thinrate=100, burnin=10000000)
#+end_src

#+RESULTS:
[[file:Figures/adaptive_trace_jax.png]]


* Scratch

Here is some in-line python code that doesn't get tangled so i can get things to work properly

** Integer overflow

Currently, due to the use of ~j**2~ in computing ~emp_var~ in the function ~adapt_step~, we program hits an integer overflow very quickly.

To demonstrate this, here is a synthetic example. In ~adapt_step~, I have (possibly temporarily) added ~emp_var~ as an output, so we can take a look.

#+begin_src python :session example :results output :tangle no
d = 2
n = 1000
key = jax.random.PRNGKey(seed=1)
keys = rand.split(key,n)
state0 = (2000000, jnp.zeros(2), jnp.ones(2), jnp.identity(2), False)
sigma = jnp.array([[2.0,1.0],[1.0,2.0]])
Q, R = qr(sigma)

def step(carry, _):
    nextstate = adapt_step(carry, Q, R, keys[carry[0]])
    return(nextstate[0], nextstate[1])

results = jl.scan(step, state0, jnp.zeros(1))
emp_var = results[1]
print(emp_var)
#+end_src

#+RESULTS:
: [[[ 4.9927820e-07 -7.2178846e-10]
:   [-7.2178846e-10  4.9927820e-07]]]

#+begin_src python :session example :results output :tangle no
print(cholesky(emp_var))
#+end_src

#+RESULTS:
: [[[ 7.0659618e-04  0.0000000e+00]
:   [-1.0215007e-06  7.0659549e-04]]]

Actually, this seems fine... seems like JAX properly converts the type, so I'm back to square one (pun intended).

indeed, in this case it actually accepted, which seems rare looking at actual runs.

#+begin_src python :session example :results output :tangle no
print(results[0])
#+end_src

#+RESULTS:
: (Array(2000001, dtype=int32, weak_type=True), Array([-0.00013929,  0.00116391], dtype=float32), Array([0.9998607, 1.001164 ], dtype=float32), Array([[ 1.0000000e+00, -1.6212175e-07],
:        [-1.6212175e-07,  1.0000013e+00]], dtype=float32), Array(True, dtype=bool))

#+begin_src python :session example :results output :tangle no
staten = (2000000, jnp.zeros(2), jnp.ones(2), jnp.array([[1e8, 0],[0,1e8]]), False)
results = jl.scan(step, staten, jnp.zeros(1))
#+end_src

#+RESULTS:


I can confirm that it isn't just an integer overflow by doing a low d sample which should be big enough  to overflow

#+begin_src python :session example :results output :tangle no
large_sample = main(d=2, n=10000, thinrate=100, burnin=1000000)
#+end_src

#+RESULTS:
: The true variance of x_1 is 3.243338108062744
: The empirical sigma value is 3.2412893772125244
: The b value is 1.000004529953003
: The computation took 2.7593398094177246 seconds

i can try even bigger maybe?


#+begin_src python :session example :results output :tangle no
larger_sample = main(d=2, n=10000, thinrate=100, burnin=10000000)
#+end_src

#+RESULTS:
: The true variance of x_1 is 0.8742398619651794
: The empirical sigma value is 0.8771054744720459
: The b value is 1.0000009536743164
: The computation took 9.534143924713135 seconds

/even then/ it is completely fine!

A smaller high-dimensional sample works too



#+begin_src python :session example :results output :tangle no
small_highd_sample = main(d=100, n=1000, thinrate=10, burnin=10000)
#+end_src

#+RESULTS:
: The true variance of x_1 is 78.75906372070312
: The empirical sigma value is 1.057357668876648
: The b value is 2.3883228302001953
: The computation took 9.417072057723999 seconds

ah we seem to have the issue? I'm going to run the same in scala to see if this is simply not long enouhg of a run.

The scala output is

#+begin_quote
The true variance of x_1 is 87.24837703682367
The empirical sigma value is 2.970108488027967
The b value is 2.4138595339835565
The computation took 17.722254715 seconds
#+end_quote

so, since I know scala manages at higher samples, this simply is not a big enough sample. So, lets try slowly increasing it until it seems to break;


#+begin_src python :session example :results output :tangle no
highd_sample_2 = main(d=100, n=10000, thinrate=10, burnin=100000)
#+end_src

#+RESULTS:
: The true variance of x_1 is 101.00172363659955
: The empirical sigma value is 33.17946661595274
: The b value is 4.563056010285721
: The computation took 80.64220094680786 seconds

Given the empirical sigma is much worse than the /smaller/ sample, I conclude that something has gone wrong here. For a sanity check, the same is done in scala:

#+begin_quote
The true variance of x_1 is 87.24837703682367
The empirical sigma value is 26.630872603471182
The b value is 4.699887552562485
The computation took 182.073910841 seconds
#+end_quote

While it is clear that more samples are needed for this kind of dimension, at least the variance is bigger than 1!

We can take a closer look at what is happening. In the next step, this is what ~adapt_step~
computes for the variance;
 
#+begin_src python :session example :results output :tangle no
j = highd_sample_2[0][-1]
x_sum = highd_sample_2[2][-1]
xxt_sum = highd_sample_2[3][-1]

emp_var= (xxt_sum/j - jnp.outer(x_sum, x_sum)/(j**2))
print(emp_var[0,0])
#+end_src

#+RESULTS:
: -5.6928716

and we see, indeed, that we have negative variance. As a sanity check,

#+begin_src python :session example :results output :tangle no
print(cholesky(emp_var))
#+end_src

#+RESULTS:
: [[nan  0.  0. ...  0.  0.  0.]
:  [nan nan  0. ...  0.  0.  0.]
:  [nan nan nan ...  0.  0.  0.]
:  ...
:  [nan nan nan ... nan  0.  0.]
:  [nan nan nan ... nan nan  0.]
:  [nan nan nan ... nan nan nan]]

So it very much fails the positive definite test.

Interestingly, contrary to what I found earlier, changing ~j~ to a float initally seems to fix things;

 #+begin_src python :session example :results output :tangle no
jfloat = j.astype(float)
emp_var_float= (xxt_sum/jfloat - jnp.outer(x_sum, x_sum)/(jfloat**2))
print(jnp.all(jnp.diag(emp_var_float) > 0)) # check that all the variances are positive
#+end_src

#+RESULTS:
: True

cbut the cholesky check for positive definiteness

#+begin_src python :session example :results output :tangle no
print(cholesky(emp_var_float))
#+end_src

#+RESULTS:
: [[nan  0.  0. ...  0.  0.  0.]
:  [nan nan  0. ...  0.  0.  0.]
:  [nan nan nan ...  0.  0.  0.]
:  ...
:  [nan nan nan ... nan  0.  0.]
:  [nan nan nan ... nan nan  0.]
:  [nan nan nan ... nan nan nan]]

still fails.

Enabling 64 bit precision fixes it!!!!!

#+begin_src python :session example :results output :tangle no
jax.config.update('jax_enable_x64', True)
#+end_src
