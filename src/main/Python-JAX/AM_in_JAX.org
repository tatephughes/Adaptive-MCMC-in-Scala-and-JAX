#+TITLE: AM in Python-JAX

:BOILERPLATE:
#+PROPERTY: header-args :tangle AM_in_JAX.py
#+BIBLIOGRAPHY: Bibliography.bib
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [letterpaper]
#+OPTIONS: toc:nil
#+LATEX_HEADER: \usepackage{amsmath,amsfonts,amsthm,amssymb,bm,bbm,tikz,tkz-graph}
#+LATEX_HEADER: \usetikzlibrary{arrows}
#+LATEX_HEADER: \usetikzlibrary{bayesnet}
#+LATEX_HEADER: \usetikzlibrary{matrix}
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \newtheorem{theorem}{Theorem}[section]
#+LATEX_HEADER: \newtheorem{corollary}[theorem]{Corollary}
#+LATEX_HEADER: \newtheorem{lemma}[theorem]{Lemma}
#+LATEX_HEADER: \newtheorem{definition}[theorem]{Definition}
#+LATEX_HEADER: \newtheorem*{remark}{Remark}
#+LATEX_HEADER: \DeclareMathOperator{\E}{\mathbb E}}
#+LATEX_HEADER: \DeclareMathOperator{\prob}{\mathbb P}
#+LATEX_HEADER: \DeclareMathOperator{\var}{\mathbb V\mathrm{ar}}
#+LATEX_HEADER: \DeclareMathOperator{\cov}{\mathbb C\mathrm{ov}}
#+LATEX_HEADER: \DeclareMathOperator{\cor}{\mathbb C\mathrm{or}}
#+LATEX_HEADER: \DeclareMathOperator{\normal}{\mathcal N}
#+LATEX_HEADER: \DeclareMathOperator{\invgam}{\mathcal{IG}}
#+LATEX_HEADER: \newcommand*{\mat}[1]{\bm{#1}}
#+LATEX_HEADER: \newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
#+LATEX_HEADER: \renewcommand*{\vec}[1]{\boldsymbol{\mathbf{#1}}}
#+EXPORT_EXCLUDE_TAGS: noexport
:END:

This file _is_ the source code; everything below gets 'tangled' into ~AM_in_JAX.py~.

* Boilerplate

** Shabang

#+begin_src python :session example :results none
#!/usr/bin/env python3
#+end_src

** Imports

#+begin_src python :session example :results none
import matplotlib.pyplot as plt
import jax
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand
from jax.numpy.linalg import solve, qr, norm, eig, eigh, inv, cholesky, det
from jax.scipy.linalg import solve_triangular
import numpy as np
import time
from AM_in_JAX_tests import *
import csv
import os
import re
#+end_src

** Test Imports

The test functions are out-of-date and no longer functional

*************** TODO [#C] Update the tests for AM in JAX :projects:
*************** END

#+begin_src python :session example :results none :tangle AM_in_JAX_tests.py
import matplotlib.pyplot as plt
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand
import jax.scipy.stats as stat
from jax import vmap
from jax.numpy.linalg import solve, qr, norm, eig, inv, cholesky
import jax
import time
from AM_in_JAX import *
#+end_src

** Enable (disable) 64-bit precision

This significantly slows the code on most GPUs.

+Without this however, the precision is not enough at higher dimensions.+
With the new method of computing and updating the covariance matrix, this may no longer need to be used. 

#+begin_src python :session example :results none
jax.config.update('jax_enable_x64', False)
#+end_src

64 bit might still be necessary for very large unthinned samples.

* Core Functions

Let's assume that the ~state~ is a tuple with four elements, instead of it's own class. JAX reads this as a PyTree, which it is happy to preform operations on (which wouldn't be the case if I made ~state~ a class like in the Scala version)

More specificaly, ~state = (j, x, x_mean, prop_cov)~.

** ~try_accept~

This function takes a state, a proposed move, and a log probabilty, and returns the next state, using the probability as expected.
f
It outputs the next state, updating the mean and covariance by
\begin{align*}
\vec{\overline{X}}_t &= \frac{t-1}{t} \vec{\overline{X}}_{t-1} + \frac{1}{t} \vec X_t, \\
\mat C_{t+1} &= \frac{t-1}{t} \mat C_t + \frac{s_d}{t}(t\vec{\overline{X}}_{t-1}\vec{\overline{X}}_{t-1}^{\intercal} - (t+1)\vec{\overline{X}}_t\vec{\overline{X}}_t^{\intercal} + \vec X_t\vec X_t^{\intercal} + \epsilon \mat I_d),\quad t\geq t_0.
\end{align*}

#+begin_src python :session example :results none
def try_accept(state, prop, alpha, mix, key):

  """ Accepts a proposed move from ~state~ with probability ~exp(min(0,alpha))~
  
  state -- A tuple for the state of the chain, in the format ~(j, x, x_mean, prop_cov)~
  prop -- The proposed move, x
  alpha -- The pre-calculated log of the Hastings ratio
  key -- PRNG keys
  
  return -- The next state (tuple) of the chain with updated mean and covariance
  """
  
  j        = state[0]
  x        = state[1]
  x_mean   = state[2]
  prop_cov = state[3]
  accept_count = state[4]
  d        = x.shape[0]
  
  log_prob = jnp.minimum(0.0, alpha)
  
  u = rand.uniform(key)

  x_new, is_accepted = jl.cond((jnp.log(u) < log_prob),
                               0, lambda _: (prop, 1),
                               0, lambda _: (x, 0))

  # update empirical mean
  x_mean_new = (x_mean*j + x_new)/(j+1)

  # update proposal covariance
  prop_cov_new = jnp.select(condlist   = [mix | (j<2*d), (not mix) & (j>=2*d)],
                            choicelist = [
                              prop_cov*((j-1)/j) +
                              (j*jnp.outer(x_mean-x_mean_new, x_mean-x_mean_new) +
                               jnp.outer(x_new - x_mean_new, x_new - x_mean_new)
                                 )*5.6644/(j*d),
                              prop_cov*((j-1)/j) +
                              (j*jnp.outer(x_mean-x_mean_new, x_mean-x_mean_new) +
                               jnp.outer(x_new - x_mean_new, x_new - x_mean_new) +
                               0.01*jnp.identity(d)
                                 )*5.6644/(j*d)],
                            default = 1)
  
  return((j + 1,
          x_new,
          x_mean_new,
          prop_cov_new,
          accept_count + is_accepted))
#+end_src

*** ~test_try_accept~ [OUT OF DATE]

The below code block does a few tests on the ~try_accept~ function. If the tests pass, it will return ~True~, otherwise it will throw an error.

#+begin_src python :session example :results none :tangle AM_in_JAX_tests.py
def test_try_accept():
    
    d = 10
    key = jax.random.PRNGKey(seed=2)
    keys = rand.split(key,10000)
    state0 = (0, jnp.zeros(10), jnp.zeros(10), jnp.identity(10), False)
    prop = jnp.ones(10)
    
    '''
    Test 1:
    if alpha=log(0.5), then the function should accept approx. 50% of the proposals
    '''
    assert jnp.abs(jnp.mean(jl.map(lambda x: try_accept(state0, prop, jnp.log(0.5), x), keys)[4]) - 0.5 < 0.1), "Accepting at unexpected rate"

    '''
    Test 1.5:
    if alpha=-0.33333333, then the function should accept approx. 0.7165 of the proposals
    '''
    assert jnp.abs(jnp.mean(jl.map(lambda x: try_accept(state0, prop, -0.3333333, x), keys)[4]) - 0.7165 < 0.1), "Accepting at unexpected rate"

    '''
    Test 2:
    if alpha=log(0)=-inf, then the function should never accept, and should return the
    proposed value
    '''
    assert jnp.all(try_accept(state0, prop, jnp.log(0), key)[1]==jnp.zeros(10)), "Not rejecting proposal"

    '''
    Test 3:
    if alpha=log(1)=0 then the function should always accept, and should return the
    proposed value
    '''
    assert jnp.all(try_accept(state0, prop, jnp.log(1), key)[1]==prop), "Not accepting proposal"

    '''
    Test 4:
    No matter what, j should increment by exactly 1
    '''
    assert jnp.all(jl.map(lambda x: try_accept(state0, prop, jnp.log(0.5), x), keys)[0]==1), "Index not correctly implemented"

    '''
    Test 5:
    When it accepts, the x_sum should increase accordingly
    '''
    assert jnp.all(try_accept(state0, prop, jnp.log(1), key)[2]==prop), "Not increased x_sum"
    assert jnp.all(try_accept(state0, prop, jnp.log(0), key)[2]==jnp.zeros(10)), "Not increased x_sum"

    '''
    Test 6:
    When it accepts, the xxt_sum should increase accordingly
    '''
    assert jnp.all(try_accept(state0, prop, jnp.log(1), key)[3]==jnp.identity(10) + jnp.outer(prop, prop)), "Not increased xxt_sum"
    assert jnp.all(try_accept(state0, prop, jnp.log(0), key)[3]==jnp.identity(10)), "Not increased xxt_sum"

    return True
#+end_src

** ~adapt_step~

This samples from the proposal distribution and computes the Hastings ratio;
\begin{align*}
q(\vec X_t^* \mid \vec X_0, \dots, X_{t-1}) \sim \mathcal N_d (\vec X_{t-1}, \mat C_t),
\end{align*}

with Hastings Ratio
\begin{align*}
\alpha = \frac12 \left[ \vec x^{\intercal} \mat \Sigma^{-1} \vec x - \vec x^{*\intercal} \mat \Sigma^{-1}\vec x^{*}\right].
\end{align*}

#+begin_src python :session example :results none
def adapt_step(state, q, r, mix, key):

    """ Samples from the current proposal distribution and computes the log Hastings Ratio, and returns the next state according to ~try_accept~

    state -- A tuple for the state of the chain, in the format ~(j, x, x_mean, prop_cov)~
    q,r -- The QR-decomposition of the target Covariance, for computing the inverse
    key -- PRNG key

    return -- The next state of the chain
    """
    
    j = state[0]
    x = state[1]
    d = x.shape[0]
    prop_cov = state[3]
    
    keys = rand.split(key,3)

    prop = jl.cond((j <= 2*d) | (mix & (rand.uniform(keys[0]) < 0.01)),
                   lambda key: rand.normal(key, shape=(d,))/(jnp.sqrt(100*d)) + x, # 'Safe' sampler
                   lambda key: rand.multivariate_normal(key, x, prop_cov), # 'Adaptive' sampler
                   keys[1])
    
    # Compute the log Hastings ratio
    alpha = 0.5 * (x.T @ (solve(r, q.T @ x))
                   - (prop.T @ solve(r, q.T @ prop)))
                   
    return(try_accept(state, prop, alpha, mix, keys[2]))
#+end_src

*** ~test_adapt_step~ [OUT OF DATE]

#+begin_src python :session example :results none :tangle AM_in_JAX_tests.py
def test_adapt_step():

    d = 2
    n = 100000
    key = jax.random.PRNGKey(seed=1)
    keys = rand.split(key,n)
    # this state was chosen being close to an actual state of the adaptive chain
    state = (100, jnp.zeros(2), jnp.array([-80.0,-5.0]), jnp.array([[260.0,100.0],[100.0,150.0]]), False)
    sigma = jnp.array([[2.0,1.0],[1.0,2.0]])
    Q, R = qr(sigma)
    
    '''
    Test 1:
    From a (hypothetical) progressed point, the result should be approximately distributed with a N(0,sigma) distribution.
    '''
    def step(carry, _):
        nextstate = adapt_step(carry, Q, R, keys[carry[0]])[0]
        return(nextstate, nextstate)
    
    assert norm(cov(jl.scan(step, state, jnp.zeros(n))[1][1]) - sigma) < 0.2, "adap_step not producing sample sufficiently close to the target distribution"

    
    return True
#+end_src


*** Covariance function

Since there isn't one built-in anywhere as far as I can tell, this is a simple function to compute the covariance matrix of a sample.

#+begin_src python :session example :results none
def cov(sample):
    
    means = jnp.mean(sample, axis=1)
    
    deviations = sample.T - means
    
    N = sample.shape[0]
    
    covariance = (deviations.T @ deviations) / (N - 1)
    
    return covariance

def mhead(M, n=3):

    return M[0:n,0:n]
#+end_src

** ~thinned_step~

 ~thinned_step~ uses a fori_loop to 'jump' steps, which JAX knows how to garbage collect. This is especially important for high dimensional samples.

#+begin_src python :session example :results none
def thinned_step(thinrate, state, q, r, mix, key):

    """Performs ~thinrate~ iterations of adapt_step, withour saving the intermiade steps"""
    
    keys = rand.split(key,thinrate)

    # I think this should scan over the keys!
    return jl.fori_loop(0, thinrate, (lambda i, x: adapt_step(x, q, r, mix, keys[i])), state)
#+end_src

*** ~test_thinned_step~

#+begin_src python :session example :results none :tangle AM_in_JAX_tests.py
def test_thinned_step():

    d = 2
    n = 1000
    thinrate = 10
    key = jax.random.PRNGKey(seed=1)
    keys = rand.split(key,n)
    # this state was chosen being close to an actual state of the adaptive chain
    state = (100, jnp.zeros(2), jnp.array([-80.0,-5.0]), jnp.array([[260.0,100.0],[100.0,150.0]]), False)
    sigma = jnp.array([[2.0,1.0],[1.0,2.0]])
    Q, R = qr(sigma)
    
    '''
    Test 1:
    the index of a state should increase by thinrate
    '''
    assert (thinned_step(thinrate, state, Q, R, keys[0])[0] == 100+thinrate), "thinned_step not correctly incrementing step count"

    return True
  
#+end_src

* Sub-Optimality Factor

Computes the 'suboptimility factor' from Roberts and Rosenthal,
$$\begin{aligned}
b = d\frac{\sum \lambda_i^{-2}}{(\sum \lambda_i^{-1})^2 },
\end{aligned}$$
where $\lamba_{i}$ are the eigenvalues of $\mat C_i^{1/2}\mat\Sigma^{-1/2}$ Currently , this is only used on the sample covariance of the generated chain, which is not how it is mean to be used. It is meant to be applied to the sampling covariance within the chain. It is also inefficient currently, and could be improved using a change of coordinate to use ~eigh~ instead of ~eig~, which would also allow it to work on GPU.

*************** TODO Fix the eigenvalue computation with a change of coordinate :projects:
*************** TODO Compute b on the sampling covariance, not the sample covariance :projects:
*************** END

I'm convinced that this is not computing the sub-optimality correctly.

#+begin_src python :session example :results none
def sub_optim_factor(sigma, sigma_j):

    """Computes the sub-optimality factor between the true target covariance ~sigma~ and the sampling covariance ~sigma_j~, from Roberts and Rosethal
    """
    
    d = sigma.shape[0]

    # looking at their code, this might be what was intended?
    lam = eig(sigma_j @ inv(sigma))[0]
    
    b = (d * sum(lam**-2) / sum(lam**-1)**2).real

    return b
#+end_src

#+begin_src python :session example :results output
def mat_sqrt(M):

    M_decomp = eig(M) # doesn't take advantage of the matrix properties!

    return M_decomp[1] @ jnp.diag(jnp.sqrt(M_decomp[0])) @ inv(M_decomp[1])
    
#+end_src

#+RESULTS:

* Plotting

Plots the trace of the first coordinate of the given sample, and saves it to a file.

#+begin_src python :session example :results none
def plot_trace(sample, file_path, j=0):

    """Plots a trace plot of the jth coordinate of the given array of states,
    and saves the figure to ~file_path~"""
    
    first = sample[:,j]
    plt.figure(figsize=(590/96,370/96))
    plt.plot(first)
    plt.title(f'Trace plot of coordinate {j}')
    plt.xlabel('Step')
    plt.ylabel('First coordinate value')
    plt.grid(True)
    plt.savefig(file_path, dpi=96)

#+end_src

* Compute time vs. dimension

** ~run_with_complexity~

This runs the main loop with an extra duration output, so that speed tests can be run

#+begin_src python :session example :results none
def run_with_complexity(sigma_d, mix, key):

    """Runs the main loop on a given target Covariance, and gets the time the main loop took.

    sigma_d -- The target covariance to sample from, usually a submatrix of ~chaotic_variance.csv~
    key -- PRNG key

    return -- A tuple containing results of the test, including the duration and suboptimality factor
    """

    Q, R = qr(sigma_d) # take the QR decomposition of sigma

    d = sigma_d.shape[0]
    
    # these numbers get good results up to d=100
    n = 1
    thinrate = 1
    burnin = 1000000

    keys = rand.split(key, n + burnin + 1)
    state0 = (2, jnp.zeros(d), jnp.zeros(d), ((0.1)**2) * jnp.identity(d)/d, 0)
    
    def step(carry, key):
        nextstate = thinned_step(thinrate, carry, Q, R, mix, key)
        return(nextstate, nextstate)

    start_time = time.time()
    
    # inital state, after burnin
    start_state = jl.fori_loop(1, burnin+1, lambda i,x: adapt_step(x, Q, R, mix, keys[i]), state0)

    # the sample
    am_sample = jl.scan(step, start_state, keys[burnin+1:])[1]

    end_time = time.time()
    duration = time.time()-start_time
    
    sigma_j = am_sample[3][-1]

    b = sub_optim_factor(sigma_d,sigma_j)

    return n, thinrate, burnin, duration, float(b) # making it into a normal float for readability
#+end_src

** ~compute_time_graph~

This goes through sub-matrices of ~sigma~ in order to make data detailing dimension against time, for plotting.

#+begin_src python :session example :results none
def compute_time_graph(sigma, mix=False, csv_file="./data/JAX_compute_times_test.csv", is_64_bit=False):

    """Loop through all the primary minors of ~sigma~ and runs the complexity test on each of them, saving the result to ~csv_file~
    """

    jax.config.update('jax_enable_x64', is_64_bit)
    
    d = sigma.shape[0]

    key = rand.PRNGKey(seed=1)
    keys = rand.split(key, d)
    
    x = range(1, d+1)
    y = jnp.array([run_with_complexity(sigma[:i,:i], mix, keys[i]) for i in x if print(i) or True])

    with open(csv_file, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerows(y)
#+end_src

* Get Sigma

Some functions to read/generate target Variance matrices for use in the tests.

#+begin_src python :session example :results none
def generate_sigma(d):

    key = jax.random.PRNGKey(seed=1)
    M = rand.normal(key, shape = (d,d))
    return inv(M @ M.T)

def read_sigma(d, file_path = './data/very_chaotic_variance.csv'):

    matrix = []
    with open(file_path, 'r', newline='') as file:
        reader = csv.reader(file)
        for row in reader:
            matrix.append([float(item) for item in row])
    return jnp.array(matrix)[0:d,0:d]
#+end_src

* Mixing Tests

To test mixing speed, we can compute ~b~ across entire un-thinned chains without burn-in.

#+begin_src python :session example :results none
def mixing_test(get_sigma = read_sigma, mix = False, csvfile = "./data/mixing_test.csv"):
    
    sigma = get_sigma(d=100)
    Q, R = qr(sigma) # take the QR decomposition of sigma
    d = sigma.shape[0]
    
    n = 100

    key = jax.random.PRNGKey(seed=1)

    sample = main(d=d, n=n, thinrate=10000, burnin=0,
                  file = "./Figures/adaptive_trace_JAX_mixing.png",
                  mix=mix, get_sigma=lambda d:sigma[0:d,0:d])

    print(sub_optim_factor(sigma, sample[3][-1]))
    
    eff_func = lambda M: sub_optim_factor(sigma, M)
    eff_vectorised = jax.vmap(eff_func)
    
    b_values = eff_vectorised(sample[3])

    y = jnp.column_stack((sample[0], b_values))
    
    with open(csvfile, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerows(y)

#+end_src

* ~main~

Due to memory constraints and garbage collection not being quite as magical, we do burn-in seperately to the main sampling.

#+begin_src python :session example :results none
def main(d=10, n=1000, thinrate=1000, burnin=0,
         write_files = False,
         trace_file = "./Figures/adaptive_trace_JAX_test.png",
         sample_file = "./data/jax_sample",
         mix = False,
         get_sigma = read_sigma,
         use_64 = False):

    """Runs the chain with a few diagnostics, mainly for testing. Returns a jax array containing the simulated sample.I
    """

    jax.config.update('jax_enable_x64', use_64)
    
    # the actual number of iterations is n*thin + burnin

    # keys for PRNG
    key = jax.random.PRNGKey(seed=1)
    keys = rand.split(key, n + burnin + 1)
    
    sigma = get_sigma(d=d)
    Q, R = qr(sigma) # take the QR decomposition of sigma

    # initial state before burn-in, j starts at "2" for safetys
    state0 = (2, jnp.zeros(d), jnp.zeros(d), ((0.1)**2) * jnp.identity(d)/d, 0)

    def step(carry, key):
        nextstate = thinned_step(thinrate, carry, Q, R, mix, key)
        return(nextstate, nextstate)

    start_time = time.time()
    
    # inital state, after burnin
    start_state = jl.fori_loop(1, burnin+1, lambda i,x: adapt_step(x, Q, R, mix, keys[i]), state0)

    # the sample
    sample = jl.scan(step, start_state, keys[burnin+1:])[1]

    # the time of the computation in seconds
    end_time = time.time()
    duration = time.time() - start_time
    
    # The final sampling covariance
    sigma_j = sample[3][-1] / (5.6644/d)
    acc_rate = sample[4][-1] / (n*thinrate+burnin)

    # According to Roberts and Rosethal, this value should go to 1.
    b1 = sub_optim_factor(sigma, jnp.identity(d))
    b2 = sub_optim_factor(sigma,sigma_j)

    print(f"The optimal sampling variance of x_1 is {sigma[0,0] * (5.6644/d)}")
    print(f"The actual sampling variance of x_1 is  {sigma_j[0,0] * (5.6644/d)}")
    print(f"The initial b value is {b1}")
    print(f"The final b value is {b2}")
    print(f"The acceptance rate is {acc_rate}")
    print(f"The computation took {duration} seconds")

    if write_files:

        # This mess writes out the sample into a format to be read by R with "source("<filename>")"

        eff_func = lambda M: sub_optim_factor(sigma, M)
        eff_vectorised = jax.vmap(eff_func)

        print("Computing the vector of b values...")
        # b_values = ', '.join([str(f) for f in eff_vectorised(sample[3])])
        b_values = ', '.join(map(str, eff_vectorised(sample[3])))
        print("Done!")
        
        print(f"Saving to the file {sample_file}...")
    
        if mix:
            if use_64:
                instance = "64_MD"
            else:
                instance = "32_MD"
        else:
            if use_64:
                instance = "64_IC"
            else:
                instance = "32_IC"

        lines = [
            f"compute_time_jax_{instance} <- {duration}",
            f"sample_jax_{instance} <- matrix(c(" + ', '.join(map(str, sample[1].flatten())) + f"), ncol={d})",
            f"bvals_jax_{instance} <- c(" + b_values + ")"
        ]
                
        with open(sample_file, 'w') as f:
            for line in lines:
                    f.write(line + "\n\n")

        print("Done!")
            
        # use np to sample the sample to the csv_file
        # np.savetxt(csv_file, sample[1], delimiter=',')

        # plot the trace of the first coordinate
        # plot_trace(sample[1], trace_file, 1)
        
    return sample

#+end_src

The entry point for if the code is run in a console.

#+begin_src python :session example :results graphics file output :file ../../../Figures/adaptive_trace_jax.png :height 200
if __name__ == "__main__":

    # This code checks wether the working  directory is correct, and if not, attemps
    # to change it.
    if not (re.search(r".*/Adaptive-MCMC-in-Scala-and-JAX$", os.getcwd())):
        os.chdir("../../../")
        if not (re.search(r".*/Adaptive-MCMC-in-Scala-and-JAX$", os.getcwd())):
            print("ERROR: Cannot find correct working directory")
        else:
            print("Succesfully found working directory")
    else:
        print("In correct working directory")
    
    #sample = main(file = "./Figures/adaptive_trace_JAX_test.png", mix = True, get_sigma=read_sigma)

    #compute_time_graph(read_sigma(d=10), "data/JAX_64bit_compute_times_laptop_test.csv")
    #mixing_test(read_sigma, mix=True,
    #            csvfile = "./data/so_factor_mixing.csv")
    #mixing_test(read_sigma, mix=False,
    #            csvfile = "./data/so_factor_not_mixing.csv")

    sample = main(d=10, n=1000, thinrate=1000, burnin=0, mix=False)

#+end_src

#+RESULTS:
[[file:../../../Figures/adaptive_trace_jax.png]]
