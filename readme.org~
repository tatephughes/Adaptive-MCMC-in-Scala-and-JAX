#+TITLE: Tangent into Adaptive MRTH
#+STARTUP: latexpreview

This is my attempt at implementing Adaptive Metropolis (or, as I prefer, Adaptive MRTH) in scala, using the breeze library.

* The toy problem

We take the toy problem from the paper "Examples of Adaptive MCMC" by Roberts and Rosenthal. 

We target the distribution $\pi(\cdot)\sim \mathcal N(0,\Sigma)$, where $\Sigma = M \in \mathbb R^{d\times d}$ is a matrix with random $\mathcal N[0,1]$ entries. Let's refresh how to use matrices in Scala by generating this matrix;

#+begin_src scala
  //silent
  import breeze.linalg._
  import breeze.stats.distributions._
  import breeze.stats.distributions.Rand.FixedSeed.randBasis

  val d = 3

  val data = Gaussian(0,1).sample(d*d).toArray.grouped(d).toArray
  
  val M = DenseMatrix(data: _*)
  val Sigma = M.t * M

  val n = 10000

#+end_src

Note that Breeze's ~DenseMatrix~ and ~DenseVector~ are actually mutable in Scala.

We'll start by reusing code from the main document to implement traditional MRTH using the proposal distribution
\begin{align*}
Q(x,\cdot)=\mathcal N(x,\lambda^{2}I_d).
\end{align*}
Recalling the algorithm, we draw from $Q$ and the compute the Hastings ratio
\begin{align*}
r(x,y) = \frac{\pi(x)q(x,y)}{\pi(y)q(y,x)}.
\end{align*}
In this case,
\begin{align*}
r(x,y) = \frac{\exp(-\frac12 x^{\intercal}\Sigma^{-1}x)\exp(-\frac12 \lambda(x-y)^{\intercal}(x-y))}{\exp(-\frac12 y^{\intercal}\Sigma^{-1}y)\exp(-\frac12 \lambda(y-x)^{\intercal}(y-x))}
\end{align*}
Taking logs to simplify,
\begin{align*}
\log r(x,y)=-\frac12 (x^{\intercal}\Sigma^{-1}x + y^{\intercal}\Sigma^{-1}y).
\end{align*}
With this in place, we can adapt our code from earlier.

We'll initialise the chain at $x_0 = 0$, and establish our tuning parameter

#+begin_src scala
  val x_0 = DenseVector.zeros[Double](d)

  val lambda = 1.0
#+end_src

We now define the 'rule' for making a single step along our Markov chain, then use ~LazyList.iterate~ to create a lazily evaluated list for 'the rest of our Markov Chain'.

#+begin_src scala
//silent
  import scala.math
  import java.util.concurrent.ThreadLocalRandom
  def rng = ThreadLocalRandom.current()



  def one_MRTH_step(x: DenseVector[Double], 
                    prop_var: DenseMatrix[Double], 
                    sigma_inv: DenseMatrix[Double]
                   ): DenseVector[Double] = {

    val proposed_move = MultivariateGaussian(x, prop_var).draw()
    val log_acceptance_prob = math.min(0.0, 0.5 * ((x.t * sigma_inv * x) - (proposed_move.t * sigma_inv * proposed_move)))
    val u = rng.nextDouble()
    if (math.log(u) < log_acceptance_prob) then proposed_move else x

  }



  def MRTH(x0: DenseVector[Double], 
           prop_var: DenseMatrix[Double], 
           sigma_inv: DenseMatrix[Double]
          ): LazyList[DenseVector[Double]] = {

    LazyList.iterate(x0)((x:DenseVector[Double]) => one_MRTH_step(x,prop_var,sigma_inv))

  }



  val mrth_sample = MRTH(x_0, DenseMatrix.eye[Double](d) :*= (lambda*lambda), inv(Sigma))
#+end_src


We should take a look at the covariance matrix of this sample. To do this, we recall that the sample variance matrix is
\begin{align*}
\mathbb Var[X] = \frac{\sum XX^{\intercal}}{n} - \frac{(\sum X)(\sum X)^{\intercal}}{n^{2}}
\end{align*}
(we want it in this form for later). To get this in Scala, we use the a similar method to what we did before

#+begin_src scala
  //silent
  val xsum = mrth_sample.take(n).foldLeft(DenseVector.zeros[Double](d))(_+_)

  val xxtvals = mrth_sample.map((x: DenseVector[Double]) => x * x.t)

  val xxtsum = xxtvals.take(n).foldLeft(DenseMatrix.zeros[Double](d,d))(_+_)

#+end_src 

And now we can get the empirical variance matrix as follows

#+begin_src scala
  //silent
  val sample_var = (xxtsum :*= 1/n.toDouble) - ((xsum * xsum.t) :*= 1/(n*n).toDouble)
#+end_src

#+begin_src scala

  println("The estimate for the Chain Variance matrix is \n" + (sample_var :*= (n.toDouble/(n.toDouble-1))))

  println("The true target variance matrix is \n" + Sigma)

#+end_src

Lets also look at the trace plot;

#+begin_src scala

  import breeze.plot._


  def plotter(sample: LazyList[DenseVector[Double]], 
              n: Int, 
              j: Int,
              file_path: String): Unit = {

    val xvals = Array.tabulate(n)(i => i.toDouble)
    val yvals = sample.map((x: DenseVector[Double]) => x(0)).take(n).toArray


    val f = Figure()
    val p = f.subplot(0)


    p += plot(xvals,yvals)
    p.xlabel = "Index"
    p.ylabel = "x_1"

    p.title = "Trace Plot of x_j"

    f.saveas(file_path)

  }

  //plotter(mrth_sample, n, 0, "./target/mdoc/Images/trace.png")

#+end_src

#+CAPTION: Trace plot of x_1
#+NAME: fig:trace1
[[file:./Images/trace.png]]

I would say, comparing these matrices, the algorithm does a reasonably good job at sampling from the target (although keep in mind, of course, that the sample variance is a biased estimator of the variance of our chain, we hope that this cleans up for high $n$). This has a very low dimension though; re-running the experiment with $d_{2}=100$ gets us the following

#+begin_src scala
  //silent

  val d_2 = 100

  val data_2 = Gaussian(0,1).sample(d_2*d_2).toArray.grouped(d_2).toArray

  val M_2 = DenseMatrix(data_2: _*)

  val Sigma_2 = M_2.t * M_2

  val lambda_2 = 0.5

  val x_0_2 = DenseVector.zeros[Double](d_2)

  val mrth_sample_2 = MRTH(x_0_2, DenseMatrix.eye[Double](d_2) :*= (lambda_2*lambda_2), inv(Sigma_2))

  val xsum_2 = mrth_sample_2.take(n).foldLeft(DenseVector.zeros[Double](d_2))(_+_)

  val xxtvals_2 = mrth_sample_2.map((x: DenseVector[Double]) => x * x.t)

  val xxtsum_2 = xxtvals_2.take(n).foldLeft(DenseMatrix.zeros[Double](d_2,d_2))(_+_)

  val sample_var_2 = (xxtsum_2 :*= 1/n.toDouble) - ((xsum_2 * xsum_2.t) :*= 1/(n*n).toDouble)
#+end_src


#+begin_src scala
  println("The estimate for the Chain Variance of x_1 is\n" + (sample_var_2(0,0) * (n.toDouble/(n.toDouble-1))))

  println("The true target variance of x_1 is \n" + Sigma_2(0,0))
#+end_src

#+begin_src scala

  //plotter(mrth_sample_2, n, 0, "./target/mdoc/Images/trace2.png")

#+end_src

APTION: Trace plot of x_1
#+NAME: fig:trace1
[[file:./Images/trace2.png]]

This is mixing terribly, as expected. Sometimes the vector never even gets accepted (the above image is generated randomly each time I export this document, so it may be flat or have a little movement). Therefore, we may be tempted to look into a better method; in comes adaptive metropolis.

#+begin_src scala
  //silent
  
      def one_AMRTH_step(state: (Int, DenseVector[Double], DenseMatrix[Double], DenseVector[Double]),
                         sigma_inv: DenseMatrix[Double]
                        ): (Int, DenseVector[Double], DenseMatrix[Double], DenseVector[Double]) = {

        val j = state._1
        val x_sum = state._2
        val xxt_sum = state._3
        val x = state._4

        val d = x.length

        if (j < 2*d) then { // procedure for n<2d

          val proposed_move = MultivariateGaussian(x, DenseMatrix.eye[Double](d) :*= ((0.01)/d.toDouble)).draw()
          val log_acceptance_prob = math.min(0.0, 0.5 * ((x.t * sigma_inv * x) - (proposed_move.t * sigma_inv * proposed_move)))
          val u = rng.nextDouble()

          if (math.log(u) < log_acceptance_prob) then {
            return((j+1, x_sum + proposed_move,  xxt_sum + (proposed_move * proposed_move.t), proposed_move))
          }  else {
            return((j+1, x_sum + x, xxt_sum + (x * x.t), x))
          }

        } else { // the actually adaptive part

          print("\n at adaptive now\n")

          val sigma_j = (xxt_sum :*= 1/j.toDouble)
                        - ((x_sum * x_sum.t) :*= (1/(j*j).toDouble))

          print(sigma_j)

          val proposed_move = (0.95) * MultivariateGaussian(x, sigma_j :*= (2.38*2.38/d.toDouble)).draw() + 0.05 * MultivariateGaussian(x, DenseMatrix.eye[Double](d) :*= ((0.01)/d.toDouble)).draw()

          val log_acceptance_prob = math.min(0.0, 0.5 * ((x.t * sigma_inv * x)
                                                  - (proposed_move.t * sigma_inv * proposed_move)))
          val u = rng.nextDouble()

          if (math.log(u) < log_acceptance_prob) then {
            print("\naccepted!\n")
            return((j+1, x_sum + proposed_move,  xxt_sum + (proposed_move * proposed_move.t), proposed_move))
          }  else {
            print("\nrejected!\n")
            return((j+1, x_sum + x, xxt_sum + (x * x.t), x))
          }
        }

      }

      def AMRTH(state0: (Int, DenseVector[Double], DenseMatrix[Double], DenseVector[Double]), 
                sigma_inv: DenseMatrix[Double]
               ): LazyList[(Int, DenseVector[Double], DenseMatrix[Double], DenseVector[Double])] = {
        LazyList.iterate(state0)((state:(Int, DenseVector[Double], DenseMatrix[Double], DenseVector[Double])) => one_AMRTH_step(state, sigma_inv))
      }

      val amrth_sample = AMRTH((0, DenseVector.zeros[Double](d), DenseMatrix.zeros[Double](d,d), DenseVector.zeros[Double](d)), inv(Sigma))

#+end_src

#+begin_src scala

  print("Hello World!")

  amrth_sample(7)._4

#+end_src
