% Created 2024-04-30 Tue 14:17
% Intended LaTeX compiler: pdflatex
\documentclass[letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{amsmath,amsfonts,amsthm,amssymb,bm,tikz,tkz-graph}
\usetikzlibrary{arrows}
\usetikzlibrary{bayesnet}
\usetikzlibrary{matrix}
\usepackage[margin=1in]{geometry}
\usepackage[english]{babel}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem*{remark}{Remark}
\DeclareMathOperator{\E}{\mathbb E}}
\DeclareMathOperator{\var}{\mathbb V\mathrm{ar}}
\DeclareMathOperator{\cov}{\mathbb C\mathrm{ov}}
\DeclareMathOperator{\cor}{\mathbb C\mathrm{or}}
\newcommand*{\mat}[1]{\bm{#1}}
\renewcommand*{\vec}[1]{\boldsymbol{\mathbf{#1}}}

%% ox-latex features:
%   !announce-start, !guess-pollyglossia, !guess-babel, !guess-inputenc,
%   maths, image, !announce-end.

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}

%% end ox-latex features


\author{Evan Tate Paterson Hughes}
\date{\today}
\title{Adaptive Metropolis in Scala and JAX}
\begin{document}

\maketitle

This is my attempt at implementing Adaptive Metropolis in Scala, using the breeze library, and python, using JAX.

This is based on the example from the article "Examples of Adaptive MCMC" by Roberts and Rosenthal.
\section{Adaptive Metropolis Algorithm}
\label{sec:orgeac033e}

A AMRTH step is defined as follows;
\begin{itemize}
\item If \(j\leq 2d\), we do a MRTH step with proposal \(q(x,\cdot)\sim \mathcal N(x,(0.1)^2I_d/d)\)
\item If \(j>2d\), we use the proposal \(q(x,\cdot)\sim(1-\beta)\mathcal N(x,(2.38)^2\Sigma_j/d)+\beta\mathcal N(x,(0.1)^2I_d/d)\), where \(\Sigma_j\) is the current empirical estimate of the covariance matrix so far.
\end{itemize}

We can compute the empirical covariance matrix at step \(j\) by

$$\begin{aligned}
\Sigma_j=\frac{{\sum_{i=0}^j} x_ix_i^{\intercal}}{j} - \frac{({\sum_{i=0}^j} x_i)({\sum_{i=0}^j} x_i)^{\intercal}}{j^2}.
\end{aligned}$$

The logic I'm using is to carry forward \(\sum x_ix_i^{\intercal}\) and \(\sum x_i\) (as well as the current index, \(j\)) as part of our 'chain', in order to compute the empirical covariance matrix as we go along (I should possibly do a \(\frac{n}{n-1}\) transormation to this matrix too), in order to sample from the proposal when \(j>2d\) .
\subsection{Measure of effectiveness}
\label{sec:org59680fb}

Roberts and Rosenthal also give the following measure of effectiveness;

$$\begin{aligned}
b = d\frac{\sum \lambda_i^{-2}}{(\sum \lambda_i^{-1})^2 }
\end{aligned}$$

where \(\lambda_i\) are the eigenvalues of \(\Sigma_p^{1/2}\Sigma^{-1/2}\) where \(\Sigma_p\) is the empirical variance matrix at the pth iteration.

\(b\) should approach 1 as the chain approaches the stationary distribution. Roughly, it measures the difference between the empirical and true variance matrices.
\section{Example Target}
\label{sec:org431c4d0}

We target the distribution \(\pi(\cdot)\sim \mathcal N(0,\Sigma)\), where \(\Sigma = M \in \mathbb R^{d\times d}\) is a matrix with random \(\mathcal N[0,1]\) entries. In Scala, this can be found as below;

\begin{verbatim}
import AdaptiveMetropolis._

// dimension of the state space
val d = 10

// create a chaotic variance to target
val data = Gaussian(0,1).sample(d*d).toArray.grouped(d).toArray
val M = DenseMatrix(data: _*)
val sigma = M.t * M
\end{verbatim}

Note that Breeze's \texttt{DenseMatrix} and \texttt{DenseVector} are actually mutable in Scala, so we need to be careful not to mutate anything.
\section{Scala implementation}
\label{sec:org48646bb}

My Scala implementation of this is found in \texttt{Main.scala} (it needs cleanup though). It is built around an object \texttt{Adaptive Metropolis}, with three methods:

\begin{itemize}
\item \texttt{AM\_step}, which takes the current state as well as the QR decomposition of the true variance, and outputs the next state of the chain.
\item \texttt{AM\_iterator}, which iterates \texttt{AM\_step} in order to create an infinite lazy list of samples.
\item \texttt{plotter}, which plots the 1st componant of the sample, and saves it to a file.
\end{itemize}


The \texttt{run} function then tests this, using \texttt{d=10}, \texttt{n=100000}, \texttt{burnin=100000} and \texttt{thinrate=10}. This function, once it finishes, prints out the true variance of \(x_1\), the empirical estimate of it from the sample, the \(b\) value, and the time the computation took. A trace plot of \(x_1\) is also saved to \texttt{Figures/adaptive\_trace\_scala.png}.
\section{JAX implementation}
\label{sec:orgf808d66}

As you might imagine, the JAX implentation is very similar, even if it is a bit more fragmented. The \texttt{AM\_step} function is split into four functions, \texttt{try\_accept}, \texttt{init\_step}, \texttt{adapt\_step}, and \texttt{AM\_step}. This is mainly due to the way JAX handles \texttt{if else} statements, making this seem like the convenient way to do it.

In the file \texttt{AM\_in\_JAX.org} (or \texttt{.md}), there is the source code as well as documentation for all the functions, but it is very similar to the scala version.
\section{Results}
\label{sec:orgd64fb55}

In both implementations, we run with \texttt{d=10}, \texttt{n=100000}, \texttt{burnin=100000} and \texttt{thinrate=10}.
\subsection{Scala}
\label{sec:org0c8008c}

The Scala output can be found using the command \texttt{sbt run} in this project's root;

\begin{quote}
The true variance of x\textsubscript{1} is 4.0203388245821

The Empirical sigma value is 4.041380732669074

The b value is 1.0000233300468546

The computation took 9.178699105 seconds
\end{quote}

\begin{itemize}
\item note: it seems to take longer now, about 12 seconds
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{./Figures/adaptive_trace_scala.png}
\end{center}

(note that I can't get rid of the transparency in Breeze-viz, so you may have to turn off dark mode to see this properly)
\subsection{JAX}
\label{sec:org1c5253e}

The JAX output can be found by running \texttt{python AM\_in\_JAX.py} in this project's root;

\begin{quote}
The true variance of x\textsubscript{1} is 9.967914581298828

The empirical sigma value is 9.99879264831543

The b value is 1.0000910758972168

The computation took 3.7597200870513916 seconds
\end{quote}

Obviously, the numbers are different since the target variance is different, but this ran over twice as fast

\begin{center}
\includegraphics[width=.9\linewidth]{./Figures/adaptive_trace_JAX.png}
\end{center}
\subsubsection{update}
\label{sec:orgc070832}

For high dimensions, I had to increase the size of the data types to 64 bit; this drastically impacted computing time for JAX

\begin{quote}
The true variance of x\textsubscript{1} is 6.589626408404064

The empirical sigma value is 6.551140424596137

The b value is 1.0000435338955926

The computation took 7.1804351806640625 seconds
\end{quote}

now, the benefits over scala are more minor!
\subsection{Very high dimensions}
\label{sec:org5a4983f}

The paper, in it's examples, get results for \texttt{d=100}. In Scala, using thinning and burn-in, the garbage collector does a good job and we can get high enough iteration counts the the program does very well.

For \texttt{d=100}, \texttt{n=10000}, \texttt{burnin=1000000}, and \texttt{thinrate=100}, I get 

\begin{quote}
The true variance of x\textsubscript{1} is 87.24837703682367

The empirical sigma value is 86.13431051648674

The b value is 1.0003412161513419

The computation took 14962.10357885 seconds
\end{quote}

\begin{center}
\includegraphics[width=.9\linewidth]{./Figures/adaptive_trace_scala_high_d.png}
\end{center}

and in JAX we again get roughly twice the speed (oops that was with about 10 times fewer iterations)

\begin{quote}
The true variance of x\textsubscript{1} is 109.05463889081547

The empirical sigma value is 111.75204879394798

The b value is 1.0036710382794376

The computation took 786.4629402160645 seconds
\end{quote}

\begin{center}
\includegraphics[width=.9\linewidth]{./Figures/adaptive_trace_jax_high_d.png}
\end{center}

(this isn't currently the correct graph, I accidentally wrote over it, I will re-run soon)
\section{Benchmarks on a more powerful system}
\label{sec:orgedf63c5}

On my personal PC with a Ryzen 7 5800X and an RTX 3060ti (notably a gaming card, not particularly suitable for this purpose) I get the following results
\subsection{Scala}
\label{sec:orge14b8e0}

\begin{quote}
The true variance of x\textsubscript{1} is 11.731161806946728
The empirical sigma value is 11.856237624615899
The b value is 1.00016514597069
The computation took 11.173750967 seconds
\end{quote}
\subsection{JAX (CPU)}
\label{sec:org770b721}

\begin{quote}
The true variance of x\textsubscript{1} is 7.858357375214536
The empirical sigma value is 8.02485487422553
The b value is 1.0004841316639048
The computation took 1.9246833324432373 seconds
\end{quote}
\subsection{JAX (GPU)}
\label{sec:orgf6d4c8e}

One of the biggest reasons to use JAX is its NVIDIA GPU support. Running the same \(d=10\) test again, 

\begin{quote}
The true variance of x\textsubscript{1} is 7.858357375214536
The empirical sigma value is 8.02485487422554
The b value is 1.0004841316639033
The computation took 30.754671812057495 seconds
\end{quote}

so something is very wrong!
\section{Complexity vs time}
\label{sec:orgfda74dc}

In order to get a better idea of how these implementations compare, we use the same chaotic variance matrix for both, with increasing submatrices, so we can make a graph of problem dimension, \texttt{d}, against time.

Firstly, here is a little python code to write out the matrix to a csv file, so both programs can read it, so we control the target variance;

\begin{verbatim}
import jax
import jax.numpy as jnp
import jax.random as rand
import csv
import numpy as np

# keys for PRNG
key = rand.PRNGKey(seed=1)

d = 100

# create a chaotic variance matrix to target
M = rand.normal(key, shape = (d,d))
sigma = M.T @ M

with open('data/chaotic_variance.csv', 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerows(np.array(sigma))

'data/chaotic_variance.csv'
\end{verbatim}

\url{data/chaotic\_variance.csv}
\subsection{Plotting}
\label{sec:org473d783}

From here, both versions have a function \texttt{compute\_time\_graph} which outputs a csv file containing the time it took to compute over a million iterations for each submatrix of the intputted variance matrix, whcih will be provided from this file. This is then plotted as below using R.

\begin{verbatim}
library(ascii)
library(ggplot2)
library(dplyr)
library(tidyr)
library(patchwork)
\end{verbatim}

\begin{verbatim}
jax_times_laptop_1 <- cbind(1:100,read.csv("./data/JAX_compute_times_laptop_1.csv", header = FALSE)) %>%
  mutate(proc = "JAX1")
names(jax_times_laptop_1) <- c("d","n", "thinrate", "burnin", "time", "b", "proc")
print(ascii(head(jax_times_laptop_1,3)), type="org")
\end{verbatim}

\phantomsection
\label{org697ca93}
\begin{verbatim}
|   | d    | n        | thinrate | burnin     | time | b    | proc |
|---+------+----------+----------+------------+------+------+------|
| 1 | 1.00 | 10000.00 | 10.00    | 1000000.00 | 1.26 | 1.00 | JAX1 |
| 2 | 2.00 | 10000.00 | 10.00    | 1000000.00 | 1.66 | 1.00 | JAX1 |
| 3 | 3.00 | 10000.00 | 10.00    | 1000000.00 | 2.20 | 1.00 | JAX1 |
\end{verbatim}


\begin{verbatim}
scala_times_laptop_1 <- cbind(1:100,read.csv("./data/scala_compute_times_laptop_1.csv", header = FALSE)) %>%
  mutate(proc = "Scala1")
names(scala_times_laptop_1) <- c("d","n", "thinrate", "burnin", "time", "b", "proc")
print(ascii(head(scala_times_laptop_1,3)), type="org")
\end{verbatim}

\phantomsection
\label{org82d9d51}
\begin{verbatim}
|   | d    | n        | thinrate | burnin     | time | b    | proc   |
|---+------+----------+----------+------------+------+------+--------|
| 1 | 1.00 | 10000.00 | 10.00    | 1000000.00 | 3.42 | 1.00 | Scala1 |
| 2 | 2.00 | 10000.00 | 10.00    | 1000000.00 | 3.52 | 1.00 | Scala1 |
| 3 | 3.00 | 10000.00 | 10.00    | 1000000.00 | 3.84 | 1.00 | Scala1 |
\end{verbatim}


We can now use \texttt{ggplot} to make a nice plot of this data.

Putting the data together and plotting

\begin{verbatim}
data <- rbind(jax_times_laptop_1, scala_times_laptop_1)
print(ascii(head(data)), type = "org")

time_graph <- ggplot(data, aes(x = d, y = time, color = proc)) +
  geom_line(size = 2) +
  scale_color_manual(values = c("JAX1" = "red", "Scala1" = "blue")) +
  theme_minimal() + 
  labs(title = "Compute Time against Dimension (Intel core i7 12700H, 16Gb RAM, Arch Linux)",
       x = "Dimension",
       y = "Compute Time (seconds)") +
  theme(text = element_text(size = 20))
print(time_graph)
\end{verbatim}

\begin{center}
\includegraphics[width=.9\linewidth]{Figures/plot_complexity_laptop_1.png}
\label{org8f6f955}
\end{center}

We can also plot the effectiveness value, \(b\);

\begin{verbatim}
b_graph <- ggplot(data, aes(x = d, y = b, color = proc)) +
  geom_line(size = 2) +
  scale_color_manual(values = c("JAX1" = "red", "Scala1" = "blue")) +
  theme_minimal() + 
  labs(title = "Effectiveness against Dimension (Intel core i7 12700H, 16Gb RAM, Arch Linux)",
       x = "Dimension",
       y = "b") +
  theme(text = element_text(size = 20))
print(b_graph)
\end{verbatim}

\begin{center}
\includegraphics[width=.9\linewidth]{Figures/plot_b_laptop.png}
\label{orgfa43065}
\end{center}

We can see that while both perform equally as well, JAX maintains roughly double the performance of Scala across the board.
\subsection{On the PC}
\label{sec:orgb4dec37}

I also ran the complexity on the PC. These are the results

\begin{verbatim}
jax_times_pc <- cbind(1:100,read.csv("./data/JAX_compute_times_pc.csv", header = FALSE)) %>%
  mutate(proc = "JAX")
names(jax_times_pc) <- c("d","n", "thinrate", "burnin", "time", "b", "proc")
scala_times_pc <- cbind(1:100,read.csv("./data/scala_compute_times_pc.csv", header = FALSE)) %>%
  mutate(proc = "Scala")
names(scala_times_pc) <- c("d","n", "thinrate", "burnin", "time", "b", "proc")
\end{verbatim}



\begin{verbatim}
data <- rbind(jax_times_pc, scala_times_pc)
print(ascii(head(data)), type = "org")

time_graph_pc <- ggplot(data, aes(x = d, y = time, color = proc)) +
  geom_line(size = 2) +
  scale_color_manual(values = c("JAX" = "red", "Scala" = "blue")) +
  theme_minimal() + 
  labs(title = "Compute Time against Dimension (Ryzen 7 5800X, 16Gb RAM, )",
       x = "Dimension",
       y = "Compute Time (seconds)") +
  theme(text = element_text(size = 20))
print(time_graph_pc)
\end{verbatim}

\begin{center}
\includegraphics[width=.9\linewidth]{./Figures/plot_complexity_pc.png}
\label{org878d697}
\end{center}

This graph is much more easily explainable; Scala is smooth and very clearly quadratic (which makes sense), and JAX is mostly quadratic but with a bump around \(d\approx60\), which is likely due to maxing out some cache size. From there the line gets a bit wobbly, and I'm not sure how to explain that.
\begin{enumerate}
\item {\bfseries\sffamily DONE} Add specs to the titles\hfill{}\textsc{projects}
\label{sec:orgc48b162}
\end{enumerate}
\subsection{Repeats}
\label{sec:org45794b3}

Given the odd shape of the original graph, I re-ran them three more times recently. A few things were different this time;
\begin{itemize}
\item I have recently re-installed my OS, meaning drivers, background processes etc. may be different
\item Python is now running in a virtual environment
\item the \texttt{eig} functions in the JAX \texttt{effectiveness} function are changed to \texttt{eigh} to take advantage of symmetry and to allow compatibility with a GPU
\end{itemize}

None of these should, hypothetically, affect anything. Nevertheless, this is the graph I get;

\begin{verbatim}
jax_times_laptop_2 <- cbind(1:100,read.csv("./data/JAX_compute_times_laptop_2.csv", header = FALSE)) %>%
  mutate(proc = "JAX2")
names(jax_times_laptop_2) <- c("d","n", "thinrate", "burnin", "time", "b", "proc")
jax_times_laptop_3 <- cbind(1:100,read.csv("./data/JAX_compute_times_laptop_3.csv", header = FALSE)) %>%
  mutate(proc = "JAX3")
names(jax_times_laptop_3) <- c("d","n", "thinrate", "burnin", "time", "b", "proc")
jax_times_laptop_4 <- cbind(1:100,read.csv("./data/JAX_compute_times_laptop_4.csv", header = FALSE)) %>%
  mutate(proc = "JAX4")
names(jax_times_laptop_4) <- c("d","n", "thinrate", "burnin", "time", "b", "proc")
scala_times_laptop_2 <- cbind(1:100,read.csv("./data/scala_compute_times_laptop_2.csv", header = FALSE)) %>%
  mutate(proc = "Scala2")
names(scala_times_laptop_2) <- c("d","n", "thinrate", "burnin", "time", "b", "proc")
scala_times_laptop_3 <- cbind(1:100,read.csv("./data/scala_compute_times_laptop_3.csv", header = FALSE)) %>%
  mutate(proc = "Scala3")
names(scala_times_laptop_3) <- c("d","n", "thinrate", "burnin", "time", "b", "proc")
scala_times_laptop_4 <- cbind(1:100,read.csv("./data/scala_compute_times_laptop_4.csv", header = FALSE)) %>%
  mutate(proc = "Scala4")
names(scala_times_laptop_4) <- c("d","n", "thinrate", "burnin", "time", "b", "proc")
\end{verbatim}

\begin{verbatim}
data <- rbind(jax_times_laptop_2, jax_times_laptop_3, jax_times_laptop_4,
              scala_times_laptop_2, scala_times_laptop_3, scala_times_laptop_4)

time_graph_laptop_2 <- ggplot(data, aes(x = d, y = time, color = proc)) +
  geom_line(size = 2) +
  scale_color_manual(values = c("JAX1" = "coral", "JAX2" = "coral2",
                                "JAX3" = "coral4", "JAX4" = "coral1",
                                "Scala1" = "cadetblue", "Scala2" = "cadetblue2",
                                "Scala3" = "cadetblue3", "Scala4" = "cadetblue4")) +
  theme_minimal() + 
  labs(title = "Compute Time against Dimension (Intel core i7 12700H, 16Gb RAM, Arch Linux)",
       x = "Dimension",
       y = "Compute Time (seconds)") +
  theme(text = element_text(size = 20))
print(time_graph_laptop_2)
\end{verbatim}

\begin{center}
\includegraphics[width=.9\linewidth]{./Figures/plot_complexity_laptop_2.png}
\label{org12a8ec8}
\end{center}

The JAX performance is significantly worse after the cache is filled, and consistently so! I have no idea what is causing this, it is the same code that was ran on the PC.
\end{document}
