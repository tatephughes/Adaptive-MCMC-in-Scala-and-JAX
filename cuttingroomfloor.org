#+TITLE: Cutting Room Floor

* Broken version(s)

We'll start by reusing code from the main document to implement traditional MRTH using the proposal distribution

$$\begin{aligned}
Q(x,\cdot)=\mathcal N(x,\lambda^{2}I_d).
\end{aligned}$$

Recalling the algorithm, we draw from $Q$ and the compute the Hastings ratio

$$\begin{aligned}
r(x,y) = \frac{\pi(x)q(x,y)}{\pi(y)q(y,x)}.
\end{aligned}$$

In this case,

$$\begin{aligned}r(x,y) = \frac{\exp(-\frac12 x^{\intercal}\Sigma^{-1}x)\exp(-\frac12 \lambda(x-y)^{\intercal}(x-y))}{\exp(-\frac12 y^{\intercal}\Sigma^{-1}y)\exp(-\frac12 \lambda(y-x)^{\intercal}(y-x))}
\end{aligned}$$

Taking logs to simplify,

$$\begin{aligned}
\log r(x,y)=-\frac12 (x^{\intercal}\Sigma^{-1}x - y^{\intercal}\Sigma^{-1}y).
\end{aligned}$$

Fistly, for comparison's sake, we do a traditional Metropolis sampler on this model.
We'll initialise the chain at $x_0 = 0$, and establish our tuning parameter

#+begin_src scala
  val x_0 = DenseVector.zeros[Double](d)

  val lambda = 1.0
#+end_src

We now define the 'rule' for making a single step along our Markov chain, then use ~LazyList.iterate~ to create a lazily evaluated list for 'the rest of our Markov Chain'.

#+begin_src scala
  import scala.math
  import java.util.concurrent.ThreadLocalRandom

  def rng = ThreadLocalRandom.current()


  def one_MRTH_step(x: DenseVector[Double], 
                    prop_var: DenseMatrix[Double], 
                    r: DenseMatrix[Double],
                    q: DenseMatrix[Double]
                   ): DenseVector[Double] = {

    val proposed_move = x.map((xi:Double) => Gaussian(xi, 0.01/d.toDouble).sample())
    val alpha = 0.5 * ((x.t * (r \ (q.t * x))) - (proposed_move.t * (r \ (q.t * proposed_move))))
    val log_acceptance_prob = math.min(0.0, alpha)
    val u = rng.nextDouble()
    if (math.log(u) < log_acceptance_prob) then proposed_move else x

  }


  def MRTH(x0: DenseVector[Double], 
           prop_var: DenseMatrix[Double], 
           sigma: DenseMatrix[Double]
          ): LazyList[DenseVector[Double]] = {

    LazyList.iterate(x0)((x:DenseVector[Double]) => one_MRTH_step(x,prop_var,sigma))

  }


  val mrth_sample = MRTH(x_0, DenseMatrix.eye[Double](d) * (lambda*lambda), inv(Sigma))
#+end_src

We should take a look at the covariance matrix of this sample. To do this, we recall that the sample variance matrix is

$$\begin{aligned}
\mathbb Var[X] = \frac{\sum XX^{\intercal}}{n} - \frac{(\sum X)(\sum X)^{\intercal}}{n^{2}}
\end{aligned}$$

(we want it in this form for later). To get this in Scala, we use the a similar method to what we did before

#+begin_src scala
  val xsum = mrth_sample.take(n).foldLeft(DenseVector.zeros[Double](d))(_+_)

  val xxtvals = mrth_sample.map((x: DenseVector[Double]) => x * x.t)

  val xxtsum = xxtvals.take(n).foldLeft(DenseMatrix.zeros[Double](d,d))(_+_)
#+end_src 

And now we can get the empirical variance matrix as follows

#+begin_src scala
  val sample_var = (xxtsum :*= 1/n.toDouble) - ((xsum * xsum.t) :*= 1/(n*n).toDouble)
#+end_src

#+begin_src scala
  println("The estimate for the Chain Variance matrix is \n" + (sample_var :*= (n.toDouble/(n.toDouble-1))))

  println("The true target variance matrix is \n" + Sigma)
#+end_src

Lets also look at the trace plot;

#+begin_src scala
  import breeze.plot._

  def plotter(sample: LazyList[DenseVector[Double]], 
              n: Int, 
              j: Int,
              file_path: String): Unit = {

    val xvals = Array.tabulate(n)(i => i.toDouble)
    val yvals = sample.map((x: DenseVector[Double]) => x(0)).take(n).toArray


    val f = Figure()
    val p = f.subplot(0)


    p += plot(xvals,yvals)
    p.xlabel = "Index"
    p.ylabel = "x_1"

    p.title = "Trace Plot of x_j"

    f.saveas(file_path)

  }

  //plotter(mrth_sample, n, 0, "./target/mdoc/Images/trace.png")

#+end_src

I would say, comparing these matrices, the algorithm does a reasonably good job at sampling from the target (although keep in mind, of course, that the sample variance is a biased estimator of the variance of our chain, we hope that this cleans up for high $n$). This has a very low dimension though; re-running the experiment with $d_{2}=100$ gets us the following

#+begin_src scala

  val d_2 = 100

  val data_2 = Gaussian(0,1).sample(d_2*d_2).toArray.grouped(d_2).toArray

  val M_2 = DenseMatrix(data_2: _*)

  val Sigma_2 = M_2.t * M_2

  val lambda_2 = 0.5

  val x_0_2 = DenseVector.zeros[Double](d_2)

  val mrth_sample_2 = MRTH(x_0_2, DenseMatrix.eye[Double](d_2) :*= (lambda_2*lambda_2), inv(Sigma_2))

  val xsum_2 = mrth_sample_2.take(n).foldLeft(DenseVector.zeros[Double](d_2))(_+_)

  val xxtvals_2 = mrth_sample_2.map((x: DenseVector[Double]) => x * x.t)

  val xxtsum_2 = xxtvals_2.take(n).foldLeft(DenseMatrix.zeros[Double](d_2,d_2))(_+_)

  val sample_var_2 = (xxtsum_2 :*= 1/n.toDouble) - ((xsum_2 * xsum_2.t) :*= 1/(n*n).toDouble)
#+end_src


#+begin_src scala
  println("The estimate for the Chain Variance of x_1 is\n" + (sample_var_2(0,0) * (n.toDouble/(n.toDouble-1))))

  println("The true target variance of x_1 is \n" + Sigma_2(0,0))
#+end_src

#+begin_src scala
  plotter(mrth_sample_2, n, 0, "./target/mdoc/Images/trace2.png")
#+end_src
 

* Class Version (unused)

** State Class definition

The Adaptive State class will contain a state of the chain as wll as a method to progress the state of the chain.

It has three attributes;

#+begin_src python :results none :tangle no
class Adaptive_State:

    def __init__(self, j, x, x_sum, xxt_sum):

        self.j       = j
        self.x       = x  
        self.x_sum   = x_sum
        self.xxt_sum = xxt_sum

#+end_src

** ~accept~ function

The ~accept~ method decides whether to accept a given proposed move, given the log-probability and a prng key.

#+begin_src python :results none :tangle no
    def accept(self, prop, alpha, key):

        log_prob = jnp.minimum(0.0, alpha)

        u = rand.uniform(key)

        new_x = prop if (jnp.log(u) < log_prob) else self.x
        
        return(Adaptive_State(
            self.j + 1,
            new_x,
            self.x_sum + new_x,
            self.xxt_sum + jnp.outer(new_x, new_x)))
    
#+end_src

** ~oneStep~ function

The main chunk, using the algorithm from Roberts and Rosenthall to make a single step to the next state.

#+begin_src python :results none :tangle no
    def oneStep(self, q, r, key):

        keys = rand.split(key,3)
        
        j       = self.j
        x       = self.x
        x_sum   = self.x_sum
        xxt_sum = self.xxt_sum
        d       = x.shape[0]

        if (j <= 2*d):

            z = rand.normal(keys[0], shape=(d,))

            # The propasal distribution is N(x,1/d) for this first stage
            prop = (z + x) * d

            # Compute the log acceptance probability
            alpha = 0.5 * (x @ (solve(r, q.T @ x))
                           - (prop @ solve(r, q.T @ prop)))
            
            return(self.accept(prop, alpha, keys[1]))
        
        else:
            
            emp_var = xxt_sum/j - jnp.outer(x_sum, x_sum.T)/j**2

            u = rand.uniform(keys[0])

            if (u < 0.95):
              prop = rand.multivariate_normal(keys[1], x, emp_var * (2.38**2/d))
            else:
              prop = ((rand.normal(keys[1], shape=(d,)) + x) * 100 * d)

            # Compute the log acceptance probability
            alpha = 0.5 * (x @ (solve(r, q.T @ x))
                           - (prop @ solve(r, q.T @ prop)))
            
            return(self.accept(prop, alpha, keys[2]))
            
#+end_src

** Testing

#+begin_src python :session example :results output :tangle no
import matplotlib.pyplot as plt
import jax.numpy as jnp
import jax.lax as jl
import jax.random as rand
import jax.scipy.stats as stat
from jax import vmap
from jax.numpy.linalg import solve, qr
import jax
import sys
sys.path.append('~/MyProjects/AdaptiveMCMC/')

from AM_in_JAX import Adaptive_State

x0 = Adaptive_State(1,jnp.array([0,0]),jnp.array([0,0]), jnp.array([[1,0],[0,1]]))

sigma = jnp.array([[2,1],[1,2]])
Q, R = qr(sigma)

key = jax.random.PRNGKey(seed=1)
keys = rand.split(key, n)

n = 1000
thinrate = 10
burnin = 1000

# Now i want to do an iterate, but I'm struggling to think of how to do this without for loops!

# I could use the scan operation 
'''
def step(carry, _):
    return(carry.oneStep(Q,R, keys[carry.j]), None)

_, results = jl.scan(step, x0, jnp.zeros(n))

results[-1].x
'''
# But Adaptive_State is not a valid JAX type. I could rewrite to not use a custom class, of course, but I'd rather not do that.

# Thinning and burnin can be done with [::thinrate] and [burnin:] I think?

#there is also jl.fori
#+end_src

#+RESULTS:




 



* Old compute sigma


In order to get a better idea of how these implementations compare, we use the same chaotic variance matrix for both, with increasing submatrices, so we can make a graph of problem dimension, ~d~, against time.

Firstly, here is a little python code to write out the matrix to a csv file, so both programs can read it, so we control the target variance;

#+begin_src python :session example :results file
import jax
import jax.numpy as jnp
import jax.random as rand
import csv
import numpy as np
from jax.numpy.linalg import solve, qr, norm, eig, eigh, inv, cholesky, det

# keys for PRNG
key = rand.PRNGKey(seed=1)

d = 100

# create a chaotic variance matrix to target
M = rand.normal(key, shape = (d,d))
sigma = inv(M @ M.T)

with open('data/very_chaotic_variance.csv', 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerows(np.array(sigma))



    
'data/chaotic_variance.csv'
#+end_src

#+RESULTS:
[[file:data/chaotic_variance.csv]]
