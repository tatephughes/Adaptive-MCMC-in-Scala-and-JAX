#+TITLE: Adaptive Metropolis in Scala and JAX

:BOILERPLATE:
#+BIBLIOGRAPHY: Bibliography.bib
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [letterpaper]
#+OPTIONS: toc:nil
#+LATEX_HEADER: \usepackage{amsmath,amsfonts,amsthm,amssymb,bm,tikz,tkz-graph}
#+LATEX_HEADER: \usetikzlibrary{arrows}
#+LATEX_HEADER: \usetikzlibrary{bayesnet}
#+LATEX_HEADER: \usetikzlibrary{matrix}
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \newtheorem{theorem}{Theorem}[section]
#+LATEX_HEADER: \newtheorem{corollary}[theorem]{Corollary}
#+LATEX_HEADER: \newtheorem{lemma}[theorem]{Lemma}
#+LATEX_HEADER: \newtheorem{definition}[theorem]{Definition}
#+LATEX_HEADER: \newtheorem*{remark}{Remark}
#+LATEX_HEADER: \DeclareMathOperator{\E}{\mathbb E}}
#+LATEX_HEADER: \DeclareMathOperator{\var}{\mathbb V\mathrm{ar}}
#+LATEX_HEADER: \DeclareMathOperator{\cov}{\mathbb C\mathrm{ov}}
#+LATEX_HEADER: \DeclareMathOperator{\cor}{\mathbb C\mathrm{or}}
#+LATEX_HEADER: \newcommand*{\mat}[1]{\bm{#1}}
#+LATEX_HEADER: \renewcommand*{\vec}[1]{\boldsymbol{\mathbf{#1}}}
#+EXPORT_EXCLUDE_TAGS: noexport
:END:

This is my attempt at implementing Adaptive Metropolis in Scala, using the breeze library, and python, using JAX.

This is based on the example from the article "Examples of Adaptive MCMC" by Roberts and Rosenthal.

* Adaptive Metropolis Algorithm

A AMRTH step is defined as follows;
- If $j\leq 2d$, we do a MRTH step with proposal $q(x,\cdot)\sim \mathcal N(x,(0.1)^2I_d/d)$
- If $j>2d$, we use the proposal $q(\vec X_t^* \mid \vec X_0, \dots, X_{t-1}) \sim \mathcal N_d (\vec X_{t-1}, \mat C_j)$, where $C_j$ is the current sampling covariance, updated as below.
  
We can compute the empirical covariance matrix at step $j$ by
\begin{align*}
\vec{\overline{X}}_t &= \frac{t-1}{t} \vec{\overline{X}}_{t-1} + \frac{1}{t} \vec X_t, \\
\mat C_{t+1} &= \frac{t-1}{t} \mat C_t + \frac{s_d}{t}(t\vec{\overline{X}}_{t-1}\vec{\overline{X}}_{t-1}^{\intercal} - (t+1)\vec{\overline{X}}_t\vec{\overline{X}}_t^{\intercal} + \vec X_t\vec X_t^{\intercal} + \epsilon \mat I_d),\quad t\geq t_0.
\end{align*}
The logic I'm using is to carry forward $\vec{\overline x}$ and $C_j$ (as well as the current index, $j$) as part of our 'chain', in order to sample from the proposal.

** Measure of effectiveness

Roberts and Rosenthal also give the following measure of effectiveness;

$$\begin{aligned}
b = d\frac{\sum \lambda_i^{-2}}{(\sum \lambda_i^{-1})^2 }
\end{aligned}$$

where $\lambda_i$ are the eigenvalues of $\Sigma_p^{1/2}\Sigma^{-1/2}$ where $\Sigma_p$ is the empirical variance matrix at the pth iteration.

$b$ should approach 1 as the chain approaches the stationary distribution. Roughly, it measures the difference between the empirical and true variance matrices.

* Example Target

We target the distribution $\pi(\cdot)\sim \mathcal N(0,\Sigma)$, where $\Sigma$ is a matrix sampled from a 'standard' inverse Wishart distribution, i.e., $\Sigma=MM^{\intercal}$, where $M\in\mathbb R^{n \times n}$ is a matrix with random $\mathcal N[0,1]$ entries. In Scala, this can be found as below;

#+begin_src scala
import AdaptiveMetropolis._

// dimension of the state space
val d = 10

// create a chaotic variance to target
val data = Gaussian(0,1).sample(d*d).toArray.grouped(d).toArray
val M = DenseMatrix(data: _*)
val sigma = M.t * M
#+end_src

Note that Breeze's ~DenseMatrix~ and ~DenseVector~ are actually mutable in Scala, so we need to be careful not to mutate anything.

* Scala implementation

My Scala implementation of this is found in ~Main.scala~ (it needs cleanup though). It is built around an object ~Adaptive Metropolis~, with three methods:

- ~AM_step~, which takes the current state as well as the QR decomposition of the true variance, and outputs the next state of the chain.
- ~AM_iterator~, which iterates ~AM_step~ in order to create an infinite lazy list of samples.
- ~plotter~, which plots the 1st componant of the sample, and saves it to a file.
  
The ~run~ function then tests this, using ~d=10~, ~n=100000~, ~burnin=100000~ and ~thinrate=10~. This function, once it finishes, prints out the true variance of $x_1$, the empirical estimate of it from the sample, the $b$ value, and the time the computation took. A trace plot of $x_1$ is also saved to ~Figures/adaptive_trace_scala.png~.

*** TODO Properly Document and Comment the Scala code :projects:

* JAX implementation

As you might imagine, the JAX implentation is very similar, even if it is a bit more fragmented. The ~AM_step~ function is split into twofunctions, ~try_accept~, and ~adapt_step~. This is mainly due to the way JAX handles ~if else~ statements, making this seem like the convenient way to do it.

In the file ~AM_in_JAX.org~, there is the source code as well as documentation for all the functions, but it is very similar to the scala version.

* Results

In all implementations, we run with ~d=10~, ~n=100000~, ~burnin=100000~ and ~thinrate=10~.

** R

As a baseline, I have also included an R version. At the root of this project, there is an R script that can be ran. The results are as follows

#+begin_quote
The true variance of x_1 is 14.1301614453525

The empirical sigma value is 14.1117375348109

The b value is 1.00003160696836

The computation took 72.3016312122345 seconds
#+end_quote

#+ATTR_ORG: :height 100
[[file:./Figures/adaptive_trace_r_d_10.png]]

** Scala

The Scala output can be found using the command ~sbt run~ in this project's root;

#+begin_quote
The true variance of x_1 is 11.731161806946728

The empirical sigma value is 11.803664855741761

The b value is 1.0000659032541865

The computation took 8.548220083 seconds
#+end_quote

#+ATTR_ORG: :height 100
[[file:./Figures/adaptive_trace_scala_d_10.png]]

(note that I can't get rid of the transparency in Breeze-viz, so you may have to turn off dark mode to see this properly)

** JAX

The JAX output can be found by running ~python AM_in_JAX.py~ in this project's root;

#+begin_quote
The true variance of x_1 is 8.333649635314941

The empirical sigma value is 8.369915962219238

The b value is 1.0001994371414185

The computation took 3.0340700149536133 seconds
#+end_quote

Obviously, the numbers are different since the target variance is different (this will be addressed shortly), but this ran nearly three times as fast! R performs relatively very slowly.

#+ATTR_ORG: :height 100
[[file:./Figures/adaptive_trace_JAX_d_10.png]]


** Very high dimensions

The paper, in it's examples, get results for ~d=100~.

In Scala, using thinning and burn-in, the garbage collector does a good job and we can get high enough iteration counts the the program does very well.

For ~d=100~, ~n=10000~, ~burnin=1000000~, and ~thinrate=100~, I get 

#+begin_quote
The true variance of x_1 is 65.0853505983081

The empirical sigma value is 66.58915290787915

The b value is 1.0034403980298399

The computation took 1398.362186623
#+end_quote

[[file:./Figures/adaptive_trace_scala_d_100.png]]

and in JAX we get an even more significant speed boost;

#+begin_quote
The true variance of x_1 is 98.31156921386719

The empirical sigma value is 96.6253662109375

The b value is 1.0056523084640503

The computation took 90.36556029319763 seconds
#+end_quote

#+ATTR_ORG: :height 100
[[file:./Figures/adaptive_trace_JAX_d_100.png]]

(this isn't currently the correct graph, I accidentally wrote over it, I will re-run soon)

* Complexity vs time

In order to get a better idea of how these implementations compare, we use the same chaotic variance matrix for both, with increasing submatrices, so we can make a graph of problem dimension, ~d~, against time.

Firstly, here is a little python code to write out the matrix to a csv file, so both programs can read it, so we control the target variance;

#+begin_src python :session example :results file
import jax
import jax.numpy as jnp
import jax.random as rand
import csv
import numpy as np
from jax.numpy.linalg import solve, qr, norm, eig, eigh, inv, cholesky, det

# keys for PRNG
key = rand.PRNGKey(seed=1)

d = 100

# create a chaotic variance matrix to target
M = rand.normal(key, shape = (d,d))
sigma = inv(M @ M.T)

with open('data/very_chaotic_variance.csv', 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerows(np.array(sigma))

'data/chaotic_variance.csv'
#+end_src

#+RESULTS:
[[file:data/chaotic_variance.csv]]

** Plotting

From here, both versions have a function ~compute_time_graph~ which outputs a csv file containing the time it took to compute over a million iterations for each submatrix of the intputted variance matrix, whcih will be provided from this file. This is then plotted as below using R.

#+begin_src R :session example :results none
#library(ascii)
library(ggplot2)
library(dplyr)
library(tidyr)
library(patchwork)
#+end_src

#+begin_src R :session example :results output
jax_times_laptop_32 <- cbind(1:100,read.csv("./data/JAX_compute_times_laptop_1.csv", header = FALSE)) %>%
  mutate(proc = "JAX32")
names(jax_times_laptop_32) <- c("d","n", "thinrate", "burnin", "time", "b", "proc")
jax_times_laptop_64 <- cbind(1:100,read.csv("./data/JAX_64bit_compute_times_laptop_1.csv", header = FALSE)) %>%
  mutate(proc = "JAX64")
names(jax_times_laptop_64) <- c("d","n", "thinrate", "burnin", "time", "b", "proc")
print(head(jax_times_laptop_1,3))
#+end_src

#+RESULTS:
: d     n thinrate burnin      time        b  proc
: 1 1 10000       10  1e+06 0.9886949 1.000000 JAX32
: 2 2 10000       10  1e+06 1.1216428 1.000010 JAX32
: 3 3 10000       10  1e+06 1.3367074 1.000016 JAX32

#+begin_src R :session example :results output
scala_times_laptop_1 <- cbind(1:100,read.csv("./data/scala_compute_times_laptop_1.csv", header = FALSE)) %>%
  mutate(proc = "Scala")
names(scala_times_laptop_1) <- c("d","n", "thinrate", "burnin", "time", "b", "proc")
print(head(scala_times_laptop_1,3))
#+end_src

#+RESULTS:
: d     n thinrate  burnin     time        b  proc
: 1 1 10000       10 1000000 2.475539 1.000000 Scala
: 2 2 10000       10 1000000 2.623258 1.000080 Scala
: 3 3 10000       10 1000000 2.978401 1.000217 Scala

#+begin_src R :session example :results output
r_times_laptop_1 <- cbind(1:80,read.csv("./data/R_compute_times_v2_laptop_1.csv", header = FALSE)) %>%
  mutate(proc = "R")
names(r_times_laptop_1) <- c("d","n", "thinrate", "burnin", "time", "b", "proc")
print(head(r_times_laptop_1, 3))
#+end_src

#+RESULTS:
: d     n thinrate burnin     time        b proc
: 1 1 10000       10  1e+06 25.70118 1.000000    R
: 2 2 10000       10  1e+06 32.07974 1.000023    R
: 3 3 10000       10  1e+06 34.13313 1.000122    R

We can now use ~ggplot~ to make a nice plot of this data.

Putting the data together and plotting

#+begin_src R :session example :results graphics file :file Figures/plot_complexity_laptop_1.png :width 1000 :exports both
data <- rbind(jax_times_laptop_32, jax_times_laptop_64, scala_times_laptop_1, r_times_laptop_1)

time_graph <- ggplot(data, aes(x = d, y = time, color = proc)) +
  geom_line(size = 2) +
  scale_color_manual(values = c("JAX32" = "red", "JAX64" = "pink", "Scala" = "blue", "R" = "darkgreen")) +
  theme_minimal() + 
  labs(title = "Compute Time against Dimension (Intel core i7 12700H, 16Gb RAM, Arch Linux)",
       x = "Dimension",
       y = "Compute Time (seconds)") +
  theme(text = element_text(size = 20))
print(time_graph)
#+end_src

#+RESULTS:
[[file:Figures/plot_complexity_laptop_1.png]]

We can also plot the final sub-optimality factor, $b$, over all the dimensions;

#+begin_src R :session example :results graphics file :file Figures/plot_b_laptop.png :width 1000 :exports both
b_graph <- ggplot(data, aes(x = d, y = b, color = proc)) +
  geom_line(size = 2) +
  scale_color_manual(values = c("JAX32" = "red", "JAX64" = "pink", "Scala" = "blue", "R" = "darkgreen")) +
  theme_minimal() + 
  labs(title = "Effectiveness against Dimension (Intel core i7 12700H, 16Gb RAM, Arch Linux)",
       x = "Dimension",
       y = "b") +
  theme(text = element_text(size = 20))
print(b_graph)
#+end_src

#+RESULTS:
[[file:Figures/plot_b_laptop.png]]

We can see that while both perform equally as well, JAX maintains a good lead in terms of speed over both Scala and especially R.

* Mixing

This is the graph comparing the two versions of the algorithms' mixing capabilities (full description to be written)

#+begin_src R :session example :results graphics file :file ~/RoamNotes/Figures/plot_mixing.png :height 600 :width 1200 :exports both
jax_b_mixing <- cbind(1:100,read.csv("./data/so_factor_mixing.csv", header = FALSE)) %>%
  mutate(proc = "Mixing")
names(jax_b_mixing) <- c("j", "alsoj","b", "proc")
jax_b_not_mixing <- cbind(1:100,read.csv("./data/so_factor_not_mixing.csv", header = FALSE)) %>%
  mutate(proc = "Not Mixing")
names(jax_b_not_mixing) <- c("j", "alsoj","b", "proc")

data <- rbind(jax_b_mixing, jax_b_not_mixing)

plot_mixing <- ggplot(data, aes(x = j, y = b, color = proc)) +
  geom_line(size = 2) +
  scale_color_manual(values = c("Mixing" = "blue", "Not Mixing" = "red")) +
  theme_minimal() + 
  labs(title = "Mixing Factor (Intel core i7 12700H, 16Gb RAM, Arch Linux)",
       x = "j",
       y = "b") +
  theme(text = element_text(size = 20))
print(plot_mixing)
#+end_src

#+RESULTS:
[[file:~/RoamNotes/Figures/plot_mixing.png]]
